
## Course Introduction

Fact data is the biggest data that you'll ever work with in data engineering. When I worked at Netflix, there were some fact data I worked on that was 2 petabytes a day of data. So why is fact data so big compared to dimensional data?

Fact data is every event that a user can use. So you know, a company like Facebook has 2 billion users. Every user can do 1000 or 2000 events every day by all the things that they're doing. So you do 2 billion times a thousand. You get to trillions of records every day that are generated. So when you're modeling your fact data, you got to be very careful because the volume can cause your cloud bills to go up a lot.

So in this 5-hour course that I have put my heart and soul into, we are going to be covering all of the details and the nitty-gritty about how to manage both small volume and large volume fact data.

So in Day 1, we are going to be covering fact data modeling fundamentals, like what is a fact, like how to model a fact, like how to make it so you have facts that join with dimensions.

It's a lot of like Kimball data modeling in the lecture. And then in the lab, we will be covering how to build a fact data model on top of the MBA game details table in the Postgres database.

Like if you don't have Postgres setup, follow the link in the description below for the setup instructions.

In Day 2, we will be covering the blurry line between fact and dimension. So when you aggregate a fact, it can kind of behave like a dimension. And it kind of gets blurry of like what's a fact and what's a dimension. And we're going to be going over that in detail. Like how to kind of draw the line between these things. And we'll also be covering the date list data structure, which is a very powerful data structure that is used at Facebook to model user activity. So you can look at like the last 30 days of user's activity as one integer. So it compresses the data a lot. And it allows Facebook to compute monthly active users very, very, very efficiently. And there's a link in the description below for more details about the date list data structure. And then in the lab, we will be building the date list data structure and using fancy bit operations that are in Postgres. It's going to be really, really exciting. And then day three will be a deep dive on shuffle and how you can use shuffle and spark and Tino and how you kind of want to minimize it. And the way that you minimize it is through this thing called a reduced fact. So a reduced fact is a way to minimize the volume of all of your fact data by preserving the most important bits. And by building reduced facts, you can supercharge your analytical patterns. So at Facebook, I built a reduced fact framework called the long term analysis framework. And what it did was it enabled decade long analyses that used to take weeks for data pipelines to run to now be done in hours. So the way that that works is by minimizing completely eliminating shuffle so that allows your pipelines to run as fast as possible. And then in the lab, we will be covering how to build a reduced fact framework so that you can minimize and supercharge the analytics at your company.

So I'm really excited for you to check out all this content.

I love building this content. And if you want to learn more about how to build this stuff in the cloud with high success frameworks like Tino and iceberg. I highly recommend checking out the data expert.io academy in the description below.

We have all sorts of this content that is completely done in the cloud. So I hope you enjoy the course. I spent a lot of time on this and get cracking. There will be timestamps for every single section in case you want to skip to one or go to whatever pieces you want. So this is a long course. So.

## Day 1 Lecture (fundamentals of fact data) begins

So what's a fact? Most people think of a truth, something that happens, something that occurred. Right? Some sort of â€¦ evidence. Something like that. Like a big thing. You think it's an action. Maybe a user logs into an app and and most someone $30, you run a mile on your Fitbit. Even though technically running a mile is a little bit more dicey in terms of like what is a fact? Because of the fact that a mile is more of an aggregation. Because of the fact that you have all of the steps, you could think of each individual step in that mile as like the lowest granularity because you can't really break, you can't really break down a step into any smaller component.

So that's one of the things to always think about when you were thinking about okay, what is a fact? Like it should be something that really can't be, it's like atomic. Like you can't break it down into a smaller piece and running a mile with your Fitbit, you technically can, right? You can break it down into the individual steps or individual pieces of that all the way down. And so that is something to think about. Facts can appear in all sorts of layers and granularities like that, where you can aggregate up, you can aggregate down all sorts of, or you can push it back down and like be at the atomic level or aggregate up to like something like a mile. So one of the things that's really, really, really nice about facts though is they don't change. They like, it's like I took this step at this time and that's not like I can go back and change that. That's one of the beautiful things, you know like I know that's one of the things I talk about in therapy sometimes is you can't change the past and then facts are just the past. Like you can't change the fact that you logged into Facebook 30 times today. And maybe you need to have better boundaries with your phone or something like that. But what I'm trying to say here is you don't have to worry about that part that was a part of dimensions. Dimensions have this like slowly changing transitive sort of property to them that like kind of make them hard to deal with. But you don't have to worry about that with facts. Which is a beautiful thing, very beautiful thing. But facts have their own set of challenges and their own set of problems that like honestly like when I compare facts and dimensions and I had if I had a pick which one is easier, like I would say dimensions are slightly easier and mostly because of what we're about to talk about. Let's go to the next slide.

Okay, so facts are more challenging because there's a lot more of them. If you think about it like let's go back to the number of steps that you take every day. So if you have 10,000 steps a day, that's a lot more data than you as a person, right? So it's like instead of having one take, one row of data for you as a person, you have a 10,000 rows of data for every step you took. So there's a lot more data. So in it will scale way more, right? And like that's an extreme example where I would say the more of the general rule is there's gonna be between 10 and 100x the data that you would see compared to your dimensions. And the big way that you can kind of figure out how big the fact data is going to balloon is based on how many actions are taken per dimension. Like for example, when I worked at Facebook, we sent about 25 to 30 notifications a day. And so what that meant was from the two billion users that balloons to 50 billion notifications. And that's just a lot of notifications. And I know that a lot of people like turned off their notifications and all sorts of stuff like that because it is like just an excessive amount of notifications. And we're gonna be talking quite a bit about these notifications and how they worked and like kind of some of the challenges that I ran into with this very bulky fact data.

Another big thing that can be important with fact data that is less important with dimensional data is you need context for effective analysis. For example, say we sent a notification. Well, like that fact in isolation is pretty terrible. It doesn't really provide any insight. But say we had say we sent a notification and then 20 minutes later you clicked on it and then five minutes later you bought something. And we have that funnel of like sent to clicked to bot. And we have that like funnel analysis. And then we have all three of those facts in a line. And then that goes like the first one being like, oh, we sent a notification in isolation. That is worthless. But if you have those three facts together then you can have a 50 billion dollar business because that's literally all Google and Facebook are our conversion funnels. Where it's like, hey, you saw a result. You clicked on that result and you took an action. And they have optimized that really, really strongly. I mean, I don't know if y'all have seen the, where Google ran this experiment where they tried out 40 different shades of blue for the links on their website. And then one shade of blue caused people to convert and purchase a little bit more than the other ones. And that was, that made them like, I think like 10 or like, something like 10 or 100 million does some, some weird number of millions of dollars for just like freaking changing the colors of blue. And so like one of the things to think about like when you're working with fact data is what other data do we need or dimensions do we need to make this fact data more valuable? Like for example, another way to think about it is say we don't have that conversion step, that purchase step, but we have the, we have the scent and clicked. So we have like the click through rate. But what if we had another dimension? We brought in the user dimension. And we now we can see like, oh, users in Canada have a better click through rate than users in the US. And it's like, why? And that can kind of like, I kind of slowly build up kind of intuitions and have you ask more deeper questions. For your fact data. So that can be a big thing.

Another really common and annoying thing about fact data is that duplicates are going to be way more common, way, way, way more common. And duplicates can be caused by a couple different like angles of things like maybe the software engineering team they push out a bug in the logger. So every time someone clicks that actually logs two records, and that's more of a data quality error. But you can also have duplicates that are genuine. Like for example, in notifications, you can have it where you send a notification, you click on it in hour one, and then you click on it again in hour seven. And those are both genuine actions that we want to log. But we don't want to count that as two. Cause then like we could have it where it's like, oh, our click through rate is 200%, which like doesn't even make sense. Like you need to be able to dedupe a lot of these records when you're working with fact data. Because if you don't dedupe, like your metrics are going to look really weird and they're not going to be like what you expect them to be. And that is actually probably one of the most challenging parts of working with fact data is this kind of deduplication step that can really cause a lot of pain and frustration. So yeah, let's go to the next slide.

How does fact data modeling work? Well, you can think about it in two ways. On one side, there's normalized facts versus denormalized facts. And they both are very important. And I've seen both of them actually be very powerful. And I've seen both of them cause a lot of problems in both cases.

Like in the lab today, we're going to be showing how denormalized facts are causing issues. But in the slide here in a second, we're going to be talking about how normalized facts can actually cause issues. So what is the difference between normalized and denormalized? Well, so imagine instead of when you have your record of like Zach logged in at this time, but it wouldn't be Zach. It would be like user ID 17 logged in at this date. And so you would have that as a record. But maybe for your analyses, you want to not do a join. So instead, you would say Zach, 29-year-old male who lives in California made this logged in at this time. So then you bring it. You can bring in some of the other dimensions from like the other actors or objects in the fact data. And that can make it so you can just do group by. And you don't have to do a join. And that can make things faster. But it can also obviously make things duplicated. Because then it's like, OK, what if I have like 50 facts a day?

Then we just copy my, we copy that 29-year-old male in California 50 times. That doesn't sound very, very efficient. Sometimes that is the better option. And we're going to go a little bit deeper into like when that is a better option and when you want to go more than normalize route.

The key thing to remember here is normalization. Like the smaller the scale, the better normalization is going to be as your option. Because when you have normalized facts, you remove all the duplicates and you increase the data integrity. And you don't have to worry about restating stuff. So that's where normalization, especially at the smaller scale, normalization is going to be a big win. And so there is a trade-off between these two. And they both work pretty well. And so yeah, let's go a little bit deeper into this.

So one of the things that I think people get wrong here is raw logs and fact data are not the same. Logging and fact data are like inextricably linked. They're almost married where if you don't have logging right, it's very hard to get fact data right. So how are they different?

I would say a big difference is raw logs are usually owned by people who don't necessarily have really strong data skills. I know at Facebook when I work there, the raw logs are owned by software engineers who mostly are responsible for working with online systems and keeping the server running. And they don't really care quite as much about the actual log data. So that's where you can have a lot of impact as a data engineer is you can work with the software engineers to get the data logged in the correct format. Because that will make your life in the fact data modeling layer a lot nicer. So that's one of the biggest problems with fact data is you usually have ugly schemas that are from the online system. And you have other, you know, it could contain duplicates and other quality errors. The raw logs don't have any quality guarantees. The fact data is going to have longer retention, nice column names, quality guarantees. It's going to be just a lot better, like the trust in the fact data should be in order of magnitude higher than the trust in the raw logs. And that is one of the biggest things that you can do as a data engineer is being able to convert those raw logs into highly trusted fact data. You can give like facts from this angle of, think of this as a, there's a few here. You have who, where, how, what, oh, that's, that should be who, where, when, what, and how. Sorry, that, that there's a double how in there. I need to fix that.

OK, so who fields are usually pushed as IDs? You can think of this as user IDs. This could be a surface IDs. This could be like, imagine like Tesla has a car and they're driving down the road. There's the ID for you as the user. There's the ID for the car, like the car ID. And you have also, you have maybe an identifier for the operating system, all sorts of things that like help you identify who is part of the event. And then you have where, where in this case can be a couple different things. This could be a location like a country or a city or a state. But this could also be where in the app, like, are you on the home page? Are you on the profile page? Are you on like, where in the app? So this can be very similar where it can be modeled with IDs as well. But a lot of times it's not modeled with IDs. It's a model is just like the pages are like backslashware or it's like the country or things like that. How and where a lot of the times in fact data modeling are very similar because we're in this virtual world. And so it's like, he used an iPhone to make this click. So is the iPhone the where or the how? Because it was on the iPhone. But I would say the iPhone in this case would be the how and the where is going to be the where on the web page or in the app or what button they clicked. That is more the where and the how is going to be more like the methodology. And you could think of this is one where it gets close between who and how are also interesting like in the Tesla example, you could think of like the Tesla car as more model as a how as opposed to a who. But it's tricky because it is like some there's ownership there because this person owns that car. So it's like it's kind of in the middle. So definitely those two are kind of really closely linked. Then you have what we have the other ones that we need to talk about. This is going to be the event like the actual thing that happened. So like in notifications there was a funnel of events, right, you have like sent or you have generated which is like a notification that could be sent and you have the notification that is sent and you have the notification that is delivered to the device and you have the notification that's clicked on and you have the notification that's converted into an action downstream or that'd be like a like a comment, a share, a purchase, something like that. And you can think of all there's all sorts of what fields like someone made a transaction someone took a step someone did some sort of event someone through a party. There's all sorts of different events that can happen that are like what? And think of those what events usually as a atomic and because you could think of an event that's like a little bit higher level. Like for example, you have like I threw Burning Man as a party and that's like that is true that that's like an event and there's probably like, but like that is more of like a group of events cause that's like a week long thing but you could think of it as like a shorter level like I registered with the Bureau of Land Management to be okay to get the okay to throw this party. That's more atomic, right? And you could think of like all the kind of atomic level things and then those higher level components are like, okay those are more aggregations of because like you could think of Burning Man as like the all of those events that are happening at one time they can kind of all aggregate up into the whole party or like it's like a dimension almost at that point because it's like okay all of these events are tied to this one thing and it's like kind of an aggregated view of one kind of set of facts. And so but we're trying to talk about modeling individual facts so remember the the atomicity of facts is also really important. And then this last piece is also critical for facts is the when the when is gonna be almost always model as a timestamp or a date, usually a timestamp. And it says like okay I clicked on this notification at this moment in time. And so that is critical for facts like without the when like you don't know where to put it and like some things to be careful here about is that you really do want to make sure that all of your devices are logging the same time zone and this should be all be logging UTC time zone because a lot of times like when companies are moving to being more mobile first where they do a lot of the logging on the client side they do client side logging then the client side logging happens most of the time in the time zone that the client is in which is bad for when it gets pushed to the server. So you want to make sure in the client app that they are logging things in UTC not in whatever the client time zone is so that when it gets pushed to the server all of your timestamps are in the same time zone so that you can easily know like that like when they actually happened and you're not it's not all shifted because of weird time zone problems. So that's a key thing to remember when a company a lot of companies are moving that way because it is better to do things client side logging because client side logging is higher fidelity than server side logging is because you get all the interactions that a user takes not just the ones that cause a web request to happen so that's a big thing to remember when you're thinking about the when of client side logging and in fact data modeling.

Okay, so more things about facts fact data sets should have quality guarantees some of the key ones that I really like for a fact data is no duplicates or like that's one of the key things is like can you crunch all the data and duplicate it down so that like when an analyst is looking at stuff it's all kind of flattened out. You also have fields that need to be there like for example that event field like the what field and the when field the what and when field should always be not null it should always be there like if they're not there like what like you can't even you can't even analyze that data if it's no it doesn't make sense it's bogus so like you should definitely always and then like really who the who field should also have some sort of not null fact data should generally be smaller than raw logs raw logs a lot of times will log a lot of extra stuff that you don't actually need that they might need for like diagnosis or for like other like the software engineer needs it to like understand what's going on in the server and like that's not something that you need to know to understand what's going on in the business and so a lot of times you'll end up modeling that stuff away.

Yeah, in fact data should parse out hard to understand columns like a lot of the times the software engineers will pass you a column that is like a string but it's actually a JSON string that's just a blob of like nastiness and it's like really hard to even query it and so fact data should not have very many of those fact data should for the most part just be like strings and integers and decimal numbers and enumerations and it shouldn't really be as much of those complex data types. There could be some complex data types like for example when I worked at Facebook one of the complex data types we had was just an array of string which was this notification was part of these experiment groups and that was a really powerful way to like easily kind of slice and dice the notifications conversion rates based on the experiments that they were in. So it's not always the case that you shouldn't have complex data types but like you definitely shouldn't have like these massive blobs of JSON that's just like painful and that's like your analysts are going to be so sad if you just like give them the raw logs and say parse the JSON yourself that's like literally your job as a data engineer is you should be making data sets that are really fun and delightful to use and if you're not doing that then yeah you're not being a good data engineer. So definitely make sure that those things are things that you are considering when you're doing fact data modeling.

Now I wanna talk a little bit about in one of those previous slides I talked about normalization versus denormalization. And so one of the questions with fact data modeling is when should we bring dimensions in? Like instead of just like the ID but we wanna actually bring in some of the values. So I wanna talk about this network logs pipeline. I'll give you a link to this as well that you guys can read more about it.

So one of the pipelines I worked on at Netflix was this network logs pipeline and this is the biggest pipeline I ever worked on in my entire career. This pipeline was, you know, it did about two petabytes of data every single day, two petabytes of brand new data every single day and one of the things that we had and what it was was the source data set was just every network request that Netflix receives, just every single one. And so that's why it's so much data. And what we wanted to do with this data set was we wanted to see how the microservices were talking to each other because I don't know if you know but Netflix is actually like very famous for like pioneering the microservice architecture. And the microservice architecture is really great for making your development faster and making your responsibilities kind of split out and like you can really do really great things with microservices but for security they're actually kind of a mess because it's like, okay, if you have, instead of having one app that you can secure, if you have 3,000 apps, think about like all the ways that things can get hacked if you have 3,000 apps, well, this is a lot more things to worry about. And so what we were trying to do was we were trying to figure out like, okay, what are all of the apps and how do they talk to each other? And like so then we can see like, okay, if this app talks to this app, we know if this app gets hacked, we know all the other apps that it talks to. And the only way that we could do that is by looking at all the network traffic. And that was like insane. But like anyways, how it worked was, we take this network traffic and we join it with this small database of IP address data that we were able to come up with which was like just all of the IP addresses for Netflix's microservice architecture. And it worked. It actually worked and the only reason that worked is because of the fact that that data set was small enough to do a broadcast join. And a broadcast join in Spark for the people who don't know is a way that you can do a join that's very efficient. But the only way it works is if one side of the join is small and by small I mean like less than like five or six gigabytes. If it's more than five or six gigabytes then Spark is gonna have a really hard time doing a broadcast join. But one of the things that happened was we were like, oh this pipeline needs to be upgraded. And we needed it to go to IPv6 instead of IPv4 and we needed to bring in a lot more IP addresses. And then we realized like this isn't gonna work anymore. This join is no longer gonna work. We will not be able to figure out like based on IP address what the app is so that then we can, because after that join what we did was like we figured out what the app was and then we aggregated down to see like, oh this app talks to this app, this app talks to this app, right? Because it's just IP1 to IP2. You just have like two IP addresses and you just do that join. But like if you can't broadcast join it, then we, I mean we tried, we literally tried to have it run without broadcast join and do a shuffle join. And like the cost ballooned, the cost ballooned like over 10 times. And so we were like, wow, okay. So how do we solve this problem? Like how do we solve this fact data modeling problem because like we can't just like give up our security questions because we wanna upgrade the IPv6. And so what was the solution here to solving this like very difficult problem even though like we initially did solve it in like kind of a more denormalized way of solving it. So what we needed to do was we didn't need to do that join. That like what we needed was instead of doing that we needed all of the apps to just log their app. Every time that a network request happened, we just needed to log which app was receiving. So then like instead of having to do this join, we would that the app would just be in the network request data itself. So that was like really crazy that we realized that that's what we needed to do was. So what we needed people to do was adopt this thing called a sidecar proxy which what it did was every app could just adopt this sidecar proxy. So every time it received a request, it would log the app that it was and was coming from. So that would give us the app name and we wouldn't need to do a join anymore. And so we would have, but now you'll see the fact data is now denormalized though because the IP address is the identifier. And so bringing in that app information, that string information actually does denormalize the data because that is something that you can answer with a join. But since the data is so large and that join just becomes unmanageable, you need to denormalize it and log it ahead of time. And so every time that the data is logged so then you don't have to do the join at all. And that made this pipeline more efficient. I mean, it was also a massive, massive, massive, massive pain to do because then we had to go and talk to 3,000 app owners and be like, yo, could you install this sidecar proxy? And we had to like get everyone on board and then get all new apps on board as well. And that was like so many conversations, right? And that's just another great example of where a lot of the times the impact that you have as a data engineer is not writing a pipeline and it's not optimizing a pipeline. It's actually going upstream and solving the problem at the source. In this case, the problem was we aren't logging the app on our network requests and we're trying to walk around it by doing these joins and essentially putting a bandaid on the solution because we're like, wow, we really need this information, but we're going to solve it through this kind of a crazy join way. And when we realize, oh, wow, we can change how the whole company manages this data and looks at security. And that was what we ended up doing. Even though, as engineers, when we think about that, we're like, which would you rather do own a pipeline that processes 100 terabytes an hour and does a join or have 3,000 conversations with app owners? I mean, I think that a lot of data engineers are going to pick the 100 terabyte an hour pipeline because that's a lot of conversations. And what do people say no? And you have to talk with them and understand why they don't want to adopt what you want them to do. You have to have debate and persuasion and all this kind of stuff like that. But that's a beautiful part of being a data engineer is that is being able to help people understand the data practices that they need to be adopting in order to make the company better and more secure. So for me, this is like one of the more bigger impacts of things that I worked on in my career that I noticed was like, wow, this is a great way to solve this problem. So yeah, denormalization wins in this case, right? So that just shows you that sometimes denormalization is actually the solution to large scale problems and not the cause of large scale problems.

OK, so we talked a lot about logging. So how does logging fit into fact data? Logging should give you all the, pretty much all the columns that you need, except for maybe some of the dimensional columns. And those, honestly, you probably shouldn't even be in the data table to begin with. You should just have the IDs. And then people can join on those IDs to get the dimensions that they need. That could be a very big important thing. Remember, this is all in collaboration with online system engineers. They are going to be the ones who are going to be knowing a lot more about the event generation, when those events are actually being created in the app. So they're going to have a lot more context on that. As a data engineer, you probably have as much context on that.

Another big thing is, don't log everything. Log only what you really need, because a lot of times these raw logs can be very expensive, and they can cost a lot of money in the cloud. And so a lot of times companies can fall victim to this idea that we need a log stuff just in case. And so that's something that you want to not do. That's something that is very an anti-pattern and against the efficiency ethos of data engineering. Or data engineering is kind of like, we're going to give you all the answers to all of your questions in the most efficient way possible. So that's a big thing.

The last thing I want to talk about with logging here is conformance. So when you are logging your data, there should be some sort of contract or schema or shared sort of vision for things. And this can be tricky because of the fact that like, for example, at Airbnb, they, for a long time, they had a lot of their app in Ruby. And we did all of our dev in Scala. And it's like Ruby in Scala don't, you can't really just import their Ruby libraries into your Scala libraries, it doesn't work. So what you need is a middle layer between the two. So between Ruby and Scala, they shared some of the schema. So for example, like I worked in pricing and availability and I worked on some of the shared schemas of like, okay, what describes a price at Airbnb and what describes what's available. And these things were defined in what's called a thrift schema. So thrift is actually a specification that is language agnostic. And it's a way to describe schema and to describe data and functions as well. And data engineers mostly just use the schema part, I don't really use the function part as much. But like it's mostly like schema and data, you can describe that stuff. And it's in a way that is shared. So the Ruby code and the Scala code will both reference the same schema for price. And so I'll know if like the Ruby code is, if they add another column to their price data, like I will, my schema will also have that new column. And hopefully my, there'll be a test there that will break my code so that the Ruby team can't push their code until they talk to me. And they say like, hey, we're adding this new column that's gonna change the calculations. So you need to go and update your code as well. So that like I can then go update my test. And I can make sure my pipeline isn't gonna produce junk data just because the online teams decide to change how they calculate price. So that can be a beautiful way to have an integration between what the online service teams are doing. Like however they're calculating the things in the app and how you are calculating your things in the pipeline. Because if you don't have this shared schema and this shared way of talking about things, those things are destined to drift. And they will slowly and slowly kind of fall apart from each other. And that's where having this kind of shared knowledge and this shared talking can be a very, very beautiful way to keep everyone on the same page and keep everyone kind of working towards the same kind of shared vision.

Let's talk about some of the other things that is super important to think about when you're dealing with high volume fact data. Sometimes like the solution is to not work with it like to actually just filter it down and not work with all of the data that can be a very powerful way to solve these problems. So for example, one of the things I worked on at Netflix was this thing called infrastructure impact, which was a metric that showed if an AB test caused a higher S3 bill or a higher AWS bill than another experience like test and control like the test group. Oh, we're going to see 2% higher AWS costs or whatever. So the way we looked at that was we did a sample of network requests and we only needed to do like a 1% sample of the network requests and that solved our problems and we were able to build pipelines a lot faster different than the security problem because in security you can't sample because you have to know all of the possible, you have that needle in the haystack problem where it's like you can't sample. So sometimes sampling doesn't work, right? And like, especially in cases like security or like where you have like those very low probability events that you need to capture then sampling is going to essentially be a show it's going to not work at all. But for a lot of metrics and a lot of directionality things you really are going to, it's probably going to give you almost the exact same number or the exact same metric, but it just uses 100 times less compute and 100 times less storage. So in a lot of ways it's more ideal. Sampling works because of this concept called the law of large numbers, which is as you have more and more examples and more and more rows of data, it approaches more of a Gaussian distribution. And so as you get more and more data you get a diminishing return on what that distribution looks like. And I mean the law of large numbers says 30, right? If you have about 30 data points then like you start to approach that normality distribution. And so that is a very powerful technique that you can use especially when you're working with metrics that are mostly used to kind of gauge directionality of things but not like when you need to know the specific row data. Because if you need to know the specific row data that's where the sampling is not going to work.

Another great example of things that you can do with fact data, especially if it's high volume is going to be bucketing where in these cases you can bucket things on like user ID or you can bucket things on like whatever the actor usually you bucket on the who, the who IDs. And the cool thing about that is like if you need to join on that column then you can join on that column. And then it like you don't have to shuffle everything. You don't have to shuffle across the entire data set. You just have to, you just join within the buckets and then it can be a lot faster. So you can do bucket joins. That is something that we are going to cover in week five for the infrastructure track. How to do bucket joins in Spark and it helps minimize shuffle. And if you go to a sorted merge bucket join which is an SMB join, then you can actually do joins without shuffle at all because in those cases you have like both buckets are, they line up and they're sorted. So it's like a zipper. So they like come together and then it just zips down. And you don't have to, there's no, there's no shuffle that needs to happen at all. And that was a big SMB joins was a very powerful technique that I used at Facebook to really make things more efficient. It's not quite as powerful or as used nowadays because Spark kind of shuffling in Spark is like a lot nicer than shuffling in high was. So, but it still is a technique that I see in practice and I even saw at Airbnb. So it's not, it's not gone yet though. So bucketing, that's the key thing to remember. Bucketing can be a really great way. If you have a very high volume fact data set, bucketing is going to be a really powerful way that you can handle that data.

Another great question is like retention. So, so you have high volume, high volume fact data. You can't just hold onto it forever. Dimensional data, you can kind of hold onto as long as you legally are allowed to. One of the things that I noticed for fact data in big tech was the whales which are any fact data tables that were greater than 100 terabytes. They had very short retention. Usually it was like a week, maybe two weeks. And those are like the whale tables which were like, you know, those are only like maybe 2% or 3% of all the data tables. And then it was like, if it was less than 10 terabytes like in big tech it was like okay, like doesn't matter. It's the only thing that mattered was like, if it got anonymized or not. And that was because they only wanted, they cared about the legal risk. But from the efficiency perspective, they didn't really care. Obviously, depending on the company and the budget of things, these rules might change. Maybe it's at your company instead of 10 and 100, it's one and 10 or it's a hundred gigs and one terabyte or whatever. There's usually there's an order of magnitude between like where you're like, we need to make this more efficient and we need to reduce the retention on this because the cost outweighs the benefit. And that's a big thing, like as a data engineer that you should be thinking about is what is the ROI for holding on to more data? And because you'll get pushed back from data scientists all the time because they're gonna be like, oh, we need this data, we need longer retention on this data just in case. And that just in case rarely comes. Like every time I hear that just in case like, I don't know, it's almost always, that's almost always a cop out and that's like not very rarely actually something that they need to worry about. So yeah, retention, they're definitely thinking about that when you're dealing with high volume data. This is the last bit of this presentation and then we'll take a break. So I talked about how a deduplicating fact data is challenging because you can have general actual duplicates of data. And but one of the other things about it is like you have to think about the time frame. Like I know in notifications, right? Where it's like, I imagine it this way, like Facebook sends you a notification, you click on it today, but then you also click on it in a year because the notification is still in your tray and then you click on it in a year. Do we care about that duplicate? Like, do we honestly genuinely care about that duplicate probably not? So there's gonna be kind of a frame that you have to like care about duplicates of some time frame where those matter, but there's also a long tail where they don't matter. And so that's a big thing to think about is how like what is the distribution there of duplicates? And you can even look at that. You can do some analysis on the data set itself to see like, okay, where is the big chunk and like what is the way to do this? Like I found one of the things that can happen with these things, like so I worked on deduplicating the notification event data set at Facebook. And when I got there that was just like one query and it was like it took like nine hours to run and hive and it was very painful because it was master data that a lot of other data sets relied on. And so we wanted to reduce the latency of that data because it was like, it was available like way late and then all these other pipelines could only start after nine hours because they were waiting on this data. So what are some options for making or reducing the latency of these deduping? So the two big ones are gonna be streaming and microbatch is, so microbatch might be like on an hourly basis and streaming obviously is gonna be on like an even lower basis than that. So we're gonna go a little bit more into details on like how I actually deduplicated this very large volume data. Streaming allows you to capture the most duplicates in a very efficient manner because you can essentially capture the duplicates on whatever window you want and you just like say, okay, we saw this notification ID and we're gonna hold on to it. And then if we see any duplicates over the next like 15, 20, 30 minutes or whatever, then we can capture most of them that way because a large majority of the duplicates happen in a short window after the first event. And so you can capture a large majority of the duplicates with streaming in a very small window.

One of the things that was interesting about this for the requirements that I had in notifications though, was that we needed to hold on to every notification for the whole day and we needed to deduplicate throughout the entire day, not just in like a 30 minute or an hour long kind of window. And one of the things that caused was, there was just so much memory. It was used so much memory and like actually streaming didn't work. I tried doing streaming stuff on this for like six weeks and I could not get it to work. And I was like, wow, is this like an unsolvable problem? Like how do we get past this problem in notifications? And so, but streaming, especially if the volume's less because you know, keep your mind that my problem, I was working on was like 50 billion records a day. So like if your problem's less than that, you're probably gonna be in a better spot. Where streaming is gonna probably be great for your use case. Especially like if your duplication window is smaller, then streaming it can be like almost a slam dunk and it can be the one of the simplest solutions. So I'm not saying just because streaming didn't work for me, that streaming wouldn't work for you. It could definitely work for you in doing these kind of deduplication of facts. This was how I actually pulled this off of deduplicating the Facebook notification data. So use this thing called hourly micro batch dedupe. And so what this did was instead of taking nine hours, it was actually available one hour after midnight. So, and that was great. We were able to really solve this problem. But like what does this pattern even look like? It's probably kind of messy. So we're gonna go over each piece of this real quick. So the first step in hourly micro batch dedupe is you get all the data for a specific hour and then you aggregate down, right? So in this case, you would have your product ID event type. And this would be for a given hour. Like for one hour, you collect all of the data for one hour first, right? And you do that for every hour, all the way from hour zero to hour 23. You just get all of them together and you group them. And that's how, that's the first step. And what this group by does is it eliminates duplicates within an hour. So within that one, within the hour after this group by, all the duplicates are eliminated. So that's step one. Then step two, what you do is you do a full outer join between hours zero and one or hours two and three or hours four and five, et cetera, et cetera. And then what this full outer join does is, again, it makes it so that it eliminates duplicates that are across hours. So if there's a duplicate that happened in an hour zero and an hour one, this full outer join would get rid of that duplicate so that you could be able to have a dedupe two hours duplicate, or a dedupe like two hours event streaming. So then what you do is these all come together and it branches like a tree. So let me kind of show you what I mean by that. So this is kind of a diagram. So what this does is you have, you wait for your hour and then you do that group by, that's what this dedupe, hours one through eight dot dot dot. And then you do the merge, which is that full outer join query. And then after hours one and two merge and hours three and four merge, then these two merge together to do hours one and four and then you keep merging, keep merging, keep merging until finally you get down to where you have hours one and 16 and hours 17 and 24 and then they merge together and then you have your final daily dedupe data set. And yeah, this like was a lot of work, but it's a, this way is a very resilient way to dedupe fact data because like it's still batch but like it also handles data in a very, it will dedupe data in a very low latency way. So definitely, I highly recommend checking this out because this works at pretty much an arbitrary scale.

Yeah, so today's lab, what we're gonna do is we're going to be looking at some of our different kind of values here. So we are working with the MBA data set again, where we have games and game details. We're going to be looking at things like teams and games and players, a couple different things like that and we're going to be building out our fact data sets that we'll be using a lot of these data.

Congratulations on getting to the end of the first lecture in the fact data modeling course. If you are watching on the platform, make sure to switch over to the next one in the links so that you can get credit for every lecture in lab here. You don't want to just keep watching completely.

Thank you so much for getting here and make sure to like, comment and subscribe.

## Day 1 Lab (fundamentals of fact data) begins

So today we're going to be working with mostly with this table a select star from game details with this look of this table real quick. So this is our table and this is what we're going to be working with. So, one of the things that what we're trying to do is this table is actually terrible. Like there's so many things that are wrong with this table. And we're going to go over a lot of what's wrong with this table. But, um, there's Because let's just think about this for a second. So game details, like the grain, so when you're working with fact data, the grain of the table matters a lot, and the grain is going to be what is considered the lowest common denominator, like the unique identifier of this table. And for game details, in this case, we're going to be working with this, right? And so this is mostly one row here is a player and their points. So in this case, I'm pretty sure we have game ID, team ID, player ID. And then there's obviously, I think that that's pretty much it. So what we want to, this is like when we identify the grain of the table, which is like for every game, every team, every player, that's kind of the unique identifier here, right? And what we want to do is, let's just go ahead and see if there's any duplicates of this table first. So what we want to do is, we want to say, okay, this is a very common query when you're working with logs that you're going to want to run, is you identify the grain of the table and you have some sort of count and then because what we're saying is this should be unique. And then this is what we want to kind of clarify. So this query, so it takes a little bit because it's like aggregating a lot of data. So it looks like there's almost two of every record in here. So that's one of the things that can happen, right? And that was actually kind of intentional because I wanted to show people, like sometimes like when you're logging, you end up getting double data. So that's going to be one of the very first things that we want to do is we want to create a filter here to get rid of the duplicates. So that is not too crazy, right? So let's go ahead and create a thing called D-Dooped as. And then I'm just going to move him in here and then say, in this case, we can just put a star here and then so we have a row number here and then over and then partition by. And then in this case, our partitioning is going to be game ID, team ID, player ID. And we're just going to call this as a row number. And then all I want to do is I'm going to say select star from D-Dooped. So this will give us all the same columns, right, as the last table. But now we're going to have this nice little row num kind of feature as well. So at the very end here, you'll see row num, right? And so row num in this case is, we have a bunch of ones here, but like you'll see if we say like order by row num descending, you'll see that there are duplicates in this table. And we saw that with the count one kind of thing, right?

Wow, I like that this query takes a million years, like because it's like has to process all the data and like game details. So you see, here are all these row num's two. So these are all the people that are duplicated, right? So these are all like duplicate records that we want to get rid of. So in that case, all we want to do is I want to say where row num equals one. And that is going to get rid of our duplicates. So that's going to be our start query that we're going to work with, right? A lot of the times like what you're, you'll end up doing here is you're going to have like an order by here. There'll be some sort of like other thing that you order by so that you always pick the, you always pick the first row. But you know what's interesting about this data set is there is nothing to order by. And that's actually one of the other problems with this data set that we're going to solve.

So okay, so we have our dedupe kind of game data now, and we have this rolling. This is looking great. One of the things that we want to talk about here is like there's probably a lot of things in here that don't matter like for example, okay, so we have all of this data. And we want like one of the things about this fact data is that it's very denormalized, right? Because you see how like we have this like team ID and then we have like team abbreviation, team city, and we have like play ID, player name, and then like all sorts of like other kind of columns in here that like really are not really necessary. And it's interesting because we actually have both columns in this table that we don't need and columns that are missing. Because one of the things that if you remember from our fact presentation is that we need that when, right, and there's no when column in here at all. And so the when column actually is, we get that from game. So let's go ahead and get that in our, we can just probably join that here where we can say join games G. And so this join here will give us our, this is the game details details.

Game ID. So this is going to give us our, and let's call this GD, this is obnoxious, okay, so we want to keep everything from game details, but then from game we have, you'll see there's this game date, EST.

So back to your question, we can actually use this to determine like, okay, are these duplicates are they not duplicates? So in that case, we actually probably want to throw that in the order by here, say order by this, and then we'll just pick the first one, like based on the game, the game date. And if those are also the same, then like I think it comes back to, it goes back to like it doesn't matter which record we pick.

So now if we query this, let me put the game date first and format things a little bit. So if we run this, you'll see now this code is going to be looking a little bit nicer. So you see how we have, we have the, the game date, we have the game ID, we have like the team ID, and then we have like player ID, but you see like, we don't need team abbreviation and team city because remember when we're doing fact data modeling, like if you can join something cheaply, then we don't need to put any of that data in with the fact, right? And because team, how many teams are there in the MBA, 30, and like even in 100 years how many teams are going to be in the MBA, 100, like it's not ever going to be, it's never going to be like big data, right? It's never, like all the, you can put all the teams in the MBA for the next 250 years and they will all fit in Excel easily. So the fact that we have the team abbreviation and team city in this table is an abomination and we should not have those. But games is different, right? And that's one of the reasons why we are bringing in that game time, right? Because games is going to grow if we, in 250 years, how many MBA games I have played? You know, thousands, hundreds of thousands, there's going to be a lot of games. So like, not having this game time is going to be a very, it will impact our analysis of our facts a lot more than, because if we have to join that in for all the 100,000 records, this query is going to get really slow because you saw this query right now take seven seconds, right? And that's only on 10 years of data. So imagine if we were doing 250 years of data, like this is going to take a lot longer, right? It's going to grow kind of like even, like it's not even going to grow linearly, right? It's going to, it's going to be even slower than that. Let's start to think about the columns that we care about here, right? So obviously we care about game date EST and game ID because game ID is something that like, well, let's look at that table to see if there's other things from game that we want to pull in because if there is, maybe that's what we want to add, if not, like, okay. So okay, I think there is one more column from game. And then if we bring in one more column, we probably don't need a bringing game ID because all the rest of it, like, doesn't matter, right? So in this case, we have all of these kind of columns in here.

Most of these columns are aggregate columns, right? Like for example, assist home, assist web, like all of these are kind of aggregate columns. We do have some things here that I think are important though which are going to be the home team ID and the game team ID or the home team ID and the visitor team ID because we want to be able to see if a player plays better when he plays at home or when he plays away. So we need these two, but we probably aren't going to store them as columns. We're just going to use them to determine other things, right? So in this case, the other column that we care about here is season, you see there's a season here. So let's go ahead and put season in here, g.season and g.home team ID, g.visitor team ID. We're going to use these mostly to compare the team ID and game details to these team IDs to say like, is it a home, are they playing at home or are they playing away? So they'll be kind of like a Boolean that we'll end up using here. But we don't really need game ID after that because the main reason for that is every other column in here is an aggregate, right? But we can essentially just aggregate the game that happens on that day and we can get all of this data ourselves. So a lot of this is like derived. So that's pretty much the only columns that we really need from the game table. And so that means that we don't have to put game ID in there. So in that case, let's just put the other columns in here real quick. So we have season, home team ID, visitor team ID, okay. And then let's go back to game details. Just kind of look at all the columns in here. So you saw how we had a, we do need team ID here, right? So in this case, we can say team ID equals home ID, right, who's going to say, and we can say this as, so we need team ID as well. So it's not mixed these actually real quick because we have all this already. We know these are coming from game. So let's make a new column here. We can say team ID equals home team ID as, and this is playing at home. We can say like, dim is playing at home, right? That's the, because if they're, if they're equal, then, and then it also has the, the false, this will also have the false. So we actually don't really need visitor team ID. We only need home because like, there's not three teams in the NBA, right? So in that case, to make this query more efficient, we're going to get rid of visitor team ID, and get rid of visitor team ID here. And we don't need to have home team ID as a column, right? We just need, we need it to have it be this dim is playing at home, because this is a very valuable column that we can also have. So we can have team ID here now, okay. So other columns here, team abbreviation, team city, we don't need those are, we can just join on team ID, and that'll be a quick, fast, easy join.

Okay, player ID, we, we probably need player ID, and player name, it's an interesting one, right? Because the number of players in the NBA grows a little bit faster than the number of teams, but not that much faster, right? Where it's like, it's probably still one that we can just use player ID, or maybe like adding player name. I think adding player name is nice because then we don't have like the, this is a great example of where we can add a column to make the queries nicer so that people can just like know who the player is, and they don't just get some integer. So I think adding both of those is probably fair, player ID and player name, because just because player also will grow a little bit faster than a team, but it will grow less fast than game. So we'll, we'll bring in both of those. So nickname, we don't need, okay, start position, I think we do need because that is the attribute of a game because of the fact that like, for example, LeBron James, sometimes he plays small forward, and sometimes he plays power forward. And so it depends on the game, which one he's starting in. So we do need start position because that's an attribute of the player in the game. This is like a part of the fact. And then if we go down further here, we have other columns here, we'll put, we'll keep comment in for now and we might end up doing some other things with comment later.

Okay, let's go through a lot of these ones here that are probably interesting. So probably minutes, right, minutes, field goals made, field goals attempted. We don't need field goal percentage, right? That's a waste, right, because that's just FGM divided by FGA. So we don't need that. I think we can have the three pointers, right, why is this like not going down?

Okay, there we go. So we have field goal, we have the three points made, we have the three points attempted. We can get rid of that. We can do the, again, we have free throws made, free throws attempted. Our goal here, right, is we, like anything that's easy to derive, like all these percentages, like we don't care about, right? So we can have all the rebounds, OREB, DREB, and REB, right, those are all the rebound columns. And then obviously we'll just keep them all here, we'll say assist, steal, block. Why is this, why is this like not letting me, okay? Like, then we have a turnover here. So this column is dumb. So I would probably rename this TO column because of the fact that you see it, it's like blue. You see how like we're getting this like squiggly here? Because TO is actually a keyword, right, in SQL. So what we can do is TO, and then I would actually just rename this as turnovers, because that's what it actually is, so that like we can, we don't use keywords. Using keywords in your columns is bad, like imagine calling your columns select. That's like a terrible name for a column, right? So we have personal files, points, and we'll keep plus minus in there as well. So these are all the columns that I think we should keep, because they're all like fundamental nature of the fact. So this is really close now, but let's go ahead and just run this query. And because I think there is, there's one more thing that I want to show with what this is doing.

Okay, it's running, it just takes like eight seconds, right?

Wow, that's a slow one, like I did not expect this to take 15 seconds. Like, this is like not that much data, right? We might want to put like a wear clause in here somewhere to like have this be filtered down. So here we go. This is our new data set that we have, right? So now we have our game date. We have our season and we have the team and then we have dim is playing at home, right? And you'll see it does have the check, right? So that is like, there is a home team and a way team. Then you have our playwrighty start position. So you'll notice that some people have a null start position, which probably means they didn't play.

Okay, so one of the things that you'll see here is there is this comment. Is an interesting one where there is, you see, there's like a DNP, like, there's a couple different things for this comment that I think are interesting. And like, because this comment is a really hard dimension to work with, because it's like kind of like very high cardinality. But you see the first little bit there is, wow dude, my, my, okay, there we go. You'll see there's like DNP, DN, so there's three, there's NWT, DNP, and DND. Those are the three that are there. And what they stand for is did not play, which means they are sitting on the bench, they just didn't play. And then there's a DND, which is it did not dress, which means they showed up to the arena and they were there. But they weren't ever going to play because they didn't ever even like where they're a uniform, right? And where they're jersey. And then NWT means that like they weren't even in the arena. They were like not even there.

So you see here's DND, right? All these different is did not travel, right? So I honestly think that these columns here, like we want to essentially look at these together to see like maybe these are other facts that we can learn more about these players with. So I would think that like this is, this column is a great example of like a raw data column that we would want to parse. So in this case, what we can say is, I think there's a, so there's a way to do this like, so let me show you how this works. This is like a very strange Postgres thing. Well, I'm going to put one more thing in here. I'm just going to put one, I'm just going to filter this down for now so that like this query doesn't take so freaking long. 10, okay, we'll do 10, 1004 so that we can, this query should really be fast. There we go. So now it's like instant. So we're filtering down to just one day of data so that like we don't process everything at once. So one of the things you'll see here is we have this thing called position. So you see this is like equals, like, so what this is doing is like this is doing like a string position. So we're saying like is DNP in the comment, right? And you'll see it is in this comment and it's at position one, right? So in this case, we want to say this is greater than zero. And then what we want to do here is we can say coalesce this was zero because we want this to be a Boolean, right? But like when it's null, if there's no comment, we want this to be false because we know that it's not DNP because. So now if we run this, what's called this as, as dim did not play. So if we run this, there we go. So now you see how we have a, it has the check marks for those days that were dim did not play, right? And so this column is going to be way better to work with than that comment column, right? And like this is a very common thing to happen when you're working with fact data. So one of the things we want to do is we want to add in a couple more here that's like dim did not play, dim did not dress. And then we want one more here, which is dim. This is not with team. And then this is NW team. And so this will give us all three of those columns. And that should you'll see we'll have all three of them now. And then there'll be which ones are kind of checked off, right? It did not dress all sorts of things like that, right? And like one of the things about this though is that they kind of cascade on each other, right? Because of the fact that you have like, if they did not dress, they did not play. And like so these kind of all of these things kind of like cascade on each other because like, but like. So that's a one thing to think about, right? So but that is something that we can do later on. That's like, I think that like having these be like this and like just logging like the raw, like is this data here or not? And not really baking in the business rules at this point is probably the better play. And then letting analysts kind of work with these columns themselves later on is probably the the better play. So that they they can see exactly what was in the data. So that's pretty much what is in that comment column. So because we now know it's in that comment column, we can probably just not even have it, right? That's an interest, it's an interesting trade off here of like if you feel like you parsed everything or if you haven't. But I'm feeling, feeling pretty good about it. So I think we can get rid of it. So this is now looking pretty close to what we want. Let's let's go through all the rest of the columns here and just see if there's something else that might be, might be missing. So the game stuff looking great team ID, great playing at home, then you have player ID, player name, then you have their start position. And then okay, so I hate this column.

I hate this column so much like this men column, like what are you going to do with this column? Like this is like this is not a column that I would want to use, right? So I think that we probably want to change minutes here to be maybe fractional instead of this because this should be this is a string right now, right? So that's a terrible column, right? So let's go look at, let's change this, you can do a thing called split, split part.

Okay, so I think if we do this, is it that?

Okay, let's say like as minutes, right? And maybe split part two as seconds, I think that's what we want. But then we can maybe turn that into a decimal. There we go, that's exactly right. So now that is going to give us what we want. So that is essentially what we want here. I think what we can do here is kind of like, I just like to use fractions. Like I don't think putting minutes and seconds like this is, this is a great example of like, okay, when like what is the query pattern that our analysts are going to be looking for? So in this case, well, I would say is we can say, we can cast this as real. And then what we can do is we can do a plus this, we can say cast as real. And then this is going to be minutes. So I'll, I'll paste you guys this query in just a second and I'll show you like what this is going to do. So now, well, that looks like what? Oh, you're right, you're right. There's I missed the division. I was like, what? You told the right, thanks for the catch there of dividing by 60. There we go, there we go. Now this is looking better, right? So now we have like, this is now a usable column, right? When people are looking in doing analytics, now they can do things like field goals per minute. And they can do free throws per minute or rebounds per minute. And you can easily turn this into a rate that is going to be a very powerful thing that you can do with this column. That's a big thing to remember when you're doing fact data modeling is like, are the columns that you're even giving useful, right? And so now I think we're pretty close here to having what we are looking for for our table. So let's go ahead and make our DDL, because I think that's probably because I think hold up.

Okay, that is a date. Okay, good. So let's go ahead and create this DDL. So we're going to say create table, we're going to call this fact game details. So this is going to be our table here.

Okay, and so do we care, so like this is another great example of like where we want to think about each column name, right? And so this first column, do we care that it's eastern time? Probably not, it's a date. So I think the first column here is going to be game date, she's going to be a date. And then so one of the things with fact data is a lot of the times you want to label the columns, either as measures or as dimensions. So in this case, season here, like, so our game date is probably not game date, it should be dim game date, and then you have dim season, and this is an integer. And then you have dim team ID, this is an integer, right, I think, or is this a long?

Oh, I think, I think we're going to be careful there, I think, I think it should be good. I think because this is only like one billion. So I think we can say it in a year, but it's like, yeah, that one's pushing it, right? And then we have that dim is playing at home, and this is going to be a Boolean, and then let's go over again. So I like to put the more identification columns first. So we have like game dim team ID, then we should have dim player ID, she's going to be an integer, and then dim player name, she's going to be a text. So then after that is where the columns can be kind of like flipped. I think the start position is probably going to be the next good one though, like dim start position, it's a text, right, then we have all the same columns that we had like dim did not play, Boolean, dim did not dress, Boolean, dim, not with team, Boolean, okay. So great, and we have a couple other ones here, then we have minutes. So in this case, minutes is actually a measure. So in this case, a lot of times people like to put M in front of it, like M minutes, because the number of minutes that was measured, and this is going to be a real, or you can say real or a decimal, they both work, I like real here, because decimal makes you like provide like the actual like precision. So fill goal FGM should be M FGM, because that's fill goals made, it's going to be an integer, an M FGA integer, right, let's do a couple more M FG, 3M integer, M FG, FG 3A integer, right, because you want to put all these over right into what they, like so that people are aware, right, because if you have these naming conventions of like, okay, if you put dim, that means it's like, these are columns that you should filter on and group by on, and M are, these are columns that you should aggregate, and you should like do all sorts of math and stuff on, right, so because they M FTM, because they M FTA, M OREB, M DREB, and M REB, right, and then I think there are almost there, then we have M assist, M steel, M block, M turnovers, and I think, oh, we have three more, so then there's M personal files, M points, and M plus minus, okay, so great, now we have to do one more thing here, which is what is the primary key of this table, so that we can make sure that we have more guarantees on this, right, so we can say primary key, so I think in this case, we're going to have a dim game date, and then dim game date, probably dim player ID, and I mean, technically you could put team ID in there as well, but is that really necessary as a primary key, I think it is, I'd put it in there too because you might be filtering on that, and that the primary key helps create indexes, and that would be my one thing I would say why we'd put team ID in there, so let's put all three in there, because I dim team ID, dim player ID, the reason why you don't need to put team ID is because it's like, can a player be on two teams on the same day, and I think that's so that is no, like, unless someone can like switch teams halfway through a game or something like that, and I don't think that that's actually possible, and so now what we need to do is essentially do all of these over into the right kind of columns here, right, so, okay, so we created the DDL, and then what we want to do is essentially move our query with a bunch of as, right, so we're going to do the as dim game date, as dim season, as dim team ID, and remember we want to fix the ordering here, so dim tier, then we want to put player ID, player name, as dim player ID, okay, so then we had a start position, as dim start position, okay, so now we're we're close and we have start position, and then we had the playing at home, then we have all these, and then I think everything else is in order, and I don't have to freaking worry about it, yeah, okay, so then this is as m minutes, then can you do column selection mode here? Oh yeah, oh, I should have done this like before, see, oh, that's so much better, so satisfying to do that, oh wait, no, we got to do it as like, gonna put it on the other side here, right, oh no, because these are as, right, oh man, it's, you actually, you can't really, it's probably a way you can do it that way, but like, whatever, let's do it, mfga, mfg3m, I'm not gonna take that long, mfg3a, as mftm, as mfta, as moreb, as mbreb, as mreb, as mreb, as a kind of assist, so one of the, one of the big things I'm trying to illustrate here is like, yeah, like you should be changing the name of things, like if you are doing fact data modeling, because like a lot of times the names of columns that you're given are terrible, and this is a great way to fix them, okay, I think we got it here, so now what we can do is we can um, wait, we can do an insert into here, so let's go ahead and do that, so we can say up here, I'm gonna get rid of this, uh, here, oh, let me turn off column selection mode, I'm gonna get rid of this real quick, and then I want to say insert into fact game details, so now this query is gonna run, this query is gonna take like, not like 20, 30 seconds, oh there we go, it's done, there we go, I don't have to, we don't have to debate about why it's slow, even though it took two and a half minutes, it definitely should not have taken two and a half minutes, but it, because it's only like a couple thousand rows of data, but um, okay, so now we have all of our data here, and we can see all of these different columns, and one of the things that is really nice about this is now we have all of like people, like we follow all the right naming conventions, we aren't, we don't have any duplicates of like excessive things that we need, right, because uh, one of the things that like is like sad is that we lost the, um, uh, the teams, right, but if we just join teams t, right, on t dot tmyd equals gd dot tmyd, right, and then we say t dot star gd dot star, we do that, right, this is just gonna be gd dot tmyd, what, oh, oh, it's, oh, it's dim tmyd, that's why, so obviously want to model everything that way, but then, okay, so now you see we can just bring in those columns, right, and we can already, like, we can have columns, we can just bring them in, and it's really cheap because that team column is very uh, just not expensive, so that can be a very powerful way to uh, use your team, uh, or to bring teams into this, even though like we remove them from the data set, so like one of the things I wanted to show here though is like, like, okay, so we have, we have like all this data here, but like, look, let's just, I think, uh, one cool column to do here, like, like, like, let's find the player who, like, who didn't, uh, like, so as to case when, um, dim, uh, not with team, so like, let's find the player, uh, then one, and the player in the NBA who like wasn't, who like, bailed out on the most games as, um, most bailed, let's call this, what we call this, uh, bailed, um, right, so it was a group by one, order by two descending, right, now this query is like way faster, right, boom, and then you can see exactly who this guy missed 21, right, you see like, this is the number of people who like, you can see exactly the number of times, right, that they, but did that, but like, maybe it's different though, right, because you also have like count, um, you can say count one as numb games, because one of the other things that think about is like, okay, but what about that, like, kind of bail percentage, that's probably the last thing that we want to think about here, and then, then I think that will be the end of this presentation, but we say like this, right, so if we cast this as a real, this is, uh, we're gonna call this as bail percent, so we want to order by three descending instead, so now this query is really, um, better, right, so, so you see, here's our bail percentage, like, which didn't, that didn't sort, what, that definitely didn't sort, right, like, that's these numbers are, oh, it's because it's, this is four, that's why, okay, there we go, so this is probably the better way to look at it, okay, so this guy BJ Taylor, he, he has one, like, he's 100%, he's shooting 100%, so there's some people who are like half the time, 20% of the time, so, but you see the people who had like, the, the number, the high numbers, like, though, like, that, uh, they have like 20 bails, but they actually, he's had a lot more games, so it was like, for them, it was more of a volume thing, so, but you can see how like, this query that we just ran here is very powerful and we were able to answer really cool questions from this data set that like, would have been a massive pain to answer with the old, with the old, uh, data model, so this is the whole idea behind fact data modeling is, can you build data sets where you can answer questions like this really quickly, right, and like, obviously, you can do amazing things with this, like, you can say like, the number of points, right, you can say as total points, right, you can see all sorts of like, whatever kind of aggregations and stuff you want, right, you could also put in things like, dim, uh, is playing at home, and then you group by two, right, and then if we group by two in order by six, then we can see like, okay, who is the person who has the highest bail percentage, uh, when they play at home or whatever, right, so, and you can see this Elliott Williams guy because he's 100%, right, and so, um, as the whole idea here, right, is, can you make queries that are, or tables that are easy to query, fun to query, and that is, if you can do that, that is going to be a very powerful thing for you as a data engineer.

Congrats on getting to the end of the day one lab. I'm really impressed with your hands on abilities here. If you're taking this class for credit on the platform, make sure to switch out to the next link so that you can get the credit that you deserve, and good job on being hands on. So, is it a fact? Is it a dimension? Like, how do we know what is the way to, uh, really differentiate?

## Day 2 Lecture (the blurry line between fact and dimension) begins

So, one of the things that came up a lot at Facebook in terms of just like these definitions that like they sound so similar but I think they really illustrate the difference here.

So, when I worked in growth at Facebook, there is two concepts. You have dim is active and you have dim is activated. These are two different dimensions that are different dimensions that are different dimensions that are different dimensions that are different dimensions that are different dimensions that are different that are on a user object. So, for example, dim is active was based on, well, did they have any activity? I think it was like, did they show up for at least a minute on the app or they showed up for less than that but they had an engagement like they had a like a comment or a share or something like that, they either like showed up for at least a minute or they engaged in the app in some way. So, but if the dimension is based on that based on an event like they liked or commented or shared something, then is that really a dimension at that point or is it just an aggregation of facts?

And that's a great thing that I'm trying to illustrate to y'all is it's both, right? And that's where this whole modeling exercise can get really, really dicey. So, I just want to compare that with this other flag dim is activated. So, I don't know if y'all have ever had needed to take like a mental health break from social media and you decided like, well, I'm going to deactivate my Facebook account. And then what you do there is like you actually go into the app and you deactivate your account and then that puts a flag on your account that you are now deactivated, which is not an aggregation though right because like it's just one thing. And so a lot of times like that will change the state of your account from activated to deactivated and then that is an example of something that is a pure dimension. That one is going to be like because that's based that's an attribute on your user object. It's not based on the events that you generate or anything like that. So those two are very interesting because technically you can like at Facebook it was very confusing because there's another way that Facebook can work or you can deactivate your Facebook account but you can keep messenger active. And so technically in that case you can have dim is activated is false and dim is active equals true and those are like the messenger only Facebook users because they like they don't like the feed but they still want to like have the DMs of Facebook. So just letting y'all know that like these dimensions in fact definitions like they do become blurry and that's an important thing to remember. So a big thing that can like kind of you want to think about when you are like creating a dimension out of an aggregation of facts is well what is the cardinality of that dimension because a lot of times like you want to bucketize it because it. Like for example, the fact that I put I had 17 likes on Facebook and then like say we wanted to look at all the other people who liked the made 17 likes on that day on Facebook that's probably like the wrong bucket right it's probably maybe like 10 to 20. And then that's the bucket that we want to do like so you want to do this thing called bucketization when you are creating your dimensions out of your aggregated facts because then you can kind of create smaller number of buckets and then that can make your group buys more informative because a lot of times if you have extremely high cardinality. Then the data is going to look really strange because like there might even be someone in that data set who's like okay I liked a thousand 17 things today on Facebook because they were just like just just going going ham and they were on Facebook for like 12 hours and they were just like scrolling like like like like like like like like and like but they're the only one in that bucket. And so like that's where like maybe you make a bucket that's like 500 plus or 100 plus or something like that so you can capture the long tail of people who are not like so that they aren't put in a bucket by themselves because you very very rarely when you're doing these bucketizations do you want to like ever have like only one one entity in each bucket like because whenever you're doing group by right. There's kind of an implicit assumption when you use the word group by and that's that you're forming groups and if you're dimensionality is so high that like you have groups of one that's a I don't know that that that doesn't sit right with me and that's like can like it can interfere with your analyses and like a lot of times that like another thing that can happen is like if you have a group of one. You come across this other problem of like prediction predictive power where like you don't you don't have normality anymore if you have one data point or one user or one thing right you need to have at least like a couple like 30 ish right before you get that normality assumption in your data so the main thing I'm trying to say here without getting to like data sciencey and technical about it is bucketize things into you know maybe like at the very most like 10 values like so that you can see the different things usually five five is going to be probably the sweet spot five to 10 is going to be a really beautiful range of like when you're bucketizing your values that you're going to be able to kind of split them up and be able to slice and dice things in a way. That makes sense and that like how you bucketize though like is needs to be informed right where like you should be looking at the distribution of the data to see like oh what's where is like the this is where like if you do like a box and whiskers plot where you have like the median. And then you have like the like you can give it as that way where you like the you have like the the zero percentile to the 10th percentile or like zero percentile to the 25th percentile 25th to 50th 50th to 75th and then 75th plus and that would give you like four buckets and then maybe you slice it more where you do like zero to 10 and then 90 plus and then 75 to 90 and like you can do it like with that case you would have six buckets or like you have like quintiles or core. However, you want to like slice up your data like but like it should be informed from like actual like statistical distributions not just like well like I thought 17 to 30 was a good bucketization for this right and that like you kind of like just pulled it out of thin air so yeah don't do that like because a lot of times like if you do that I've been there I've been there where I pulled my bucketization out of thin air and like. And like then you like create all this data and then people are like so how did you define your buckets and then like your answer is dumb and then people like are like wow like why did you do it that way and then you have to like think about it and that's where like a lot of times though when you're bucketizing things it's good to have the value there too so that if people want to change the buckets they can go make their own case when statement and do whatever bucketization that they want to do. So like it's a trade off though because if you add if you don't bucketize then your cardinality of that is or you're going to have an extra column and that the cardinality of that column is going to be a lot higher and it's not going to compress as well so it's definitely a trade off in terms of those things but like generally speaking the flexibility of having the non bucketized value is worth it. Like because then the data scientists can play around with the exact values and they can go and make their own buckets and then maybe they decide that these buckets are better than the ones that you made and then you got to go back and backfill and like I don't know like it's that's I've been there a lot so just just letting y'all know that that's something that like you probably have to look forward to in your career.

So key thing about this slide facts dimensions they're blurry they can be based on each other so because if you think about it from the other angle like I want to just talk about the dim is activated one more time there's also the event of you going and changing that value which is an event that has a timestamp which like you mutated your activated flag at this moment in time which is a fact but it's also a dimension. Because it's like the state the state is the dimension the action is the fact so like you actually kind of have both in that case as well so just letting y'all know the big thing here is that like a lot of this the delineation between these things is kind of blurry. Well like dimensions like the big thing with dimensions is they are the things that you group on like when you run the like group by in your sequel queries that these are the values that are going to show up like user ID your country or device or gender or you know I don't know like scoring class or whatever like and these can be both high or low. Low card anality right user ID would be considered like a high cardinality dimension whereas country is kind of more like a medium cardinality dimension and then like gender is like a pretty low cardinality dimension and then uh and the last bit for dimensions like is they generally come from a snapshot of state so at Facebook at Netflix at Airbnb the main way that dimensions come about is there's a production database. And we we take a snapshot of the production database at a moment in time and whatever those values are in that at that moment in time or what the values are for that date and so they come from a snapshot of state that's like a very another important thing to think about when you come come around with dimensions.

Okay so uh facts on the other hand are going to be kind of the opposite where like these are the things that you aggregate you some or average or count or do all those things with them like and those are going to be the big ways to like think about it where they go inside the aggregation function whereas dimensions go kind of outside with the group by so. So facts are almost always higher dimension because the number of things I can do as a user of an app is usually more than one because I'm one user who can do many things and I can log in I can delete my content I can watch a video I can you know you just gotta think about how many things that one user can do and it's going to be generally speaking a lot higher than one thing and even if they can do one thing they can usually do it multiple times so the last bit. That kind of distinguishes facts and dimensions is facts come from logs they are going to be generated like when an event happens and a lot of times this is going to happen from logging so that's going to be a big thing to kind of distinguish these things but keeping in mind that you can aggregate facts and they can turn into dimensions or facts can also change dimensions on of y'all have heard of change data capture.

Change data CDC is a great example of the extremely blurry line between the two where you you kind of model a state change of a dimension as an event or as a fact and then you can kind of recreate your dimensions at any moment in time based on the stack of changes that have happened and so that the CDC is like way blurry because that's like right I feel that that's like sits right in the middle between a fact and a dimension. So yeah it's pretty cool stuff so when I worked at Airbnb I worked in pricing and availability which is a fun place to be and there's there's some there's some things that I think are interesting about price for example price as as an attribute on an Airbnb listing on like so on a specific night you have a price for an Airbnb. And is that price is that a fact or is it a dimension right and it's one of those that seems like it might be a fact right because you can sum it so you can see like okay if I bought every night at this Airbnb for a month you can sum it up and kind of see what it would be so that's one thing to think about or you can average it can count it so like that prices are doubles it's kind of rare for like a double or like a decimal number. So that's one thing to be a dimension one of the things that's interesting about this is that it kind of is a dimension though because of the fact that it's it's the it's the attribute of the night or it's like this is just the price of that night it's a state right so let's go to the next slide.

There's two things that think about here you have the host can go and change their settings where they might offer a discount like a last minute discount or like an early bird discount or something like that and when they change their settings that will log an event and that is a fact because it's logged but price is actually derived from all of the settings that the host is set and these and these settings are state and since their state price is actually a dimension and so it feels like a fact but it's actually a dimension and that's always one of the things that I like was like wow this is this is like kind of a weird thing about like how these kind of different elements of the puzzle of data engineering kind of fit together I always thought that that was so strange okay let's talk a little bit more about like these dimensions that like are based on facts we talked about dim is active right that makes sense you also like dim bought something maybe they they ever bought something right if they're someone on the website and they that's a dimension that's based on one event that means that they have not zero events in their fact purchases table or whatever it's a roll up you also have things like dim has ever booked so these are great where it's like okay did they ever show up did they were they ever labeled fake that was when dim ever labeled fake that was a dimension that I had at Facebook when I worked on the fake accounts table and that one was essentially like a feature where it was like a lot of times like if an account is ever labeled fake that is a very strong indicator of like how that account is going to behave over the long run even if it gets unlabeled fake so that is like it can be a very good dimension to bring in but again it comes back to okay how do you get dim ever labeled fake that is comes from the fact that like at some point a machine learning model created a label that said this account was fake and and so these these ones usually only ever flip from false to true you can think of these you can think of dimensions that are a little bit different like a you can say like dim is monthly active where it could flip back where it's like okay you're active one day of a month and then you're in active for 31 days and then so it flips from true and then back to false so if it has like kind of another time component to it you can have that as well that's another very powerful thing to think about so then you have days since this is something we're going to be covering in week five of the analytics track we're going to go a lot deeper into this kind of it's like this retention analytics cohort curve stuff that will be very powerful they use this a lot at Facebook you can think of it as like okay everyone who signed up for the app on this day how many of them are still active in a week in a month in a year and you can kind of see like usually it starts off very high because on the first day everyone showed up so it starts off 100% and then it kind of slows down and then it like it kind of reaches what's called an asymptote where it's like there's like 10 or 20% of the people left who are going to keep coming back and those are like the sticky people and if you all want to like look up more about that to be ready for week five look up a thing called J curve or retention analytical pattern and these are also based on these are even one layer on top of the aggregation right because it's like okay you were active on one day and then it's like how many days has it been since you were since you were active on that day or how many days has it been since you signed up and so then you can see like okay what is what does that curve look like and so and that's like even one more layer on top it's like a dimension on top of a dimension which is based on the aggregation of a fact so it gets a lot of the stuff gets very muddled that's one of the big things I'm trying to illustrate to y'all is that but that's a good thing to remember here is a lot of we have a lot of these dimensions and in our lab today we're going to be looking at dim is active let's give you one of the big ones that we look at so okay so let's talk more about a categorical fact and bucketing of dimensions right so we had the simple one we had a very simple bucketing example in week one where we bucketized users based on points right but a lot of times it might be more complicated than just like one value it's like points like for example for Airbnb super hosts there's like there's like a bunch of criteria you have to look at a bunch of columns together to create that dimension you have to have like a certain amount of ratings they have to be a certain number you have to have a certain number of bookings right and you have to have a certain amount of revenue or something like that there's like a certain number of it to look at maybe three or four columns to determine if you're a super host at Airbnb and so like it's not always like you just bucketize based on the value of one column so a lot of these things can be more complicated than that like to create these kind of more conditional columns that you can look at that are also very powerful and they can be used for your analytics and you can kind of see like oh this group is very powerful versus like these other groups and then you try to get more people into that group or like a lot of times these dimensions can end up setting like the North Star for the company where it's like okay we want to get as many people to fit this definition and you know at Airbnb that's one of their things right is they want to get as many super hosts on the platform as they can but they don't want to like make that they don't want to change the definition of that dimension because like that would defeat the purpose right because they could just game the system and be like everyone's a super host right and then that works but they have to like come up with some sort of criteria to fit people in there so that it's a meaningful dimension and not just like gaming the system so that's another thing to think about is that a lot of times when you make these dimensions one of the things that can be tricky is they can be very hard to change like for example there is another like one of the other things that I was looking at when I worked at Facebook at one time is like I don't know if y'all know but there is a hard limit on the number of friends you can have on Facebook you can only have 5,000 friends and if you it won't let you add more than that and because they realize that like well if you increase that limit like what you can't it's a one way door right it's like once you increase it you can't go back you can't like force people to be like oh like you have 5,000 friends or you have 8,000 friends and we're going back to 5K so you have to like unfriend 3000 people so like what is some of the times like when you have these dimensions they can have a very strong anchoring effect on the product like LinkedIn has the same same exact problem and it's one of the things that I hate the most about LinkedIn so LinkedIn has a connection request limit as well it's 30,000 and once you hit 30,000 you can't connect with more people which I always think is so so silly where it's like why is it 30,000 like why can it be like whatever number come on Microsoft you're a trillion dollar company I'm pretty sure y'all can figure out how to deal with people who have more than 30,000 connections but anyways that's kind of the idea is bucketization and dimension definitions like a lot of times once these are set it's hard to change them I know another example for me when I was working at Airbnb and this is one of the biggest impacts I had at Airbnb was when I was working there we had a column called dim is available which is can this Airbnb like is this Airbnb available on this night and one of the things that's weird about that is that the old definition of that was kind of did a host set a rule that blocks this night that's that was the old definition and one of the things I did was I made a slight change to that which was can a trip be booked that contains this night so those are slightly different definitions and like that was a I mean now just making that change was a two year undertaking at Airbnb just to make that change even though the those definitions are almost the same they're about three or four percent different because of like some edge cases like one of them being sandwich nights which is where like a host says okay you have to book it book at least three nights then if there's a reservation in two days then technically tonight is unavailable because you have to book three nights in a row so these nights are like squished in and they're unavailable whereas in the old definition they were marked as available so like it's a great example of like when you determine the definition of a dimension changing that especially if it's used throughout the company can be very painful and can take a long time and so that was one of the big things I worked on was making that change and getting the new definition of availability adopted throughout the company and that was a fun problem but also a massive pain like I was like holy crap like why is something so simple so annoying to deal with so definitely when you are thinking about these definitions get as many people involved don't just like go with something and be like done like really try to think about how these impact the business and that can make a big difference on how many times you have to change them because know that changing these dimensions is not cheap it's actually very expensive so big thing to remember when you are doing your categorical dimension values well here's another great thing to think about is should you use dimensions or facts to analyze users like so do we care more about how many activated users there are or how many active users there are and I mean like the answer is probably both I think that like there's an intuition here probably from most of the people in this presentation that active users is probably the more valuable of the two but activated also matters because you can especially like if you in a lot of times you might use both of these together because imagine if you have active users divided by activated users so you can see the percent of users who signed up who are active that's a beautiful analysis right that's another great metric that you could look at that I think could be a very powerful metric right so the thing that you want to think about here is what's the difference between like sign ups and growth right sign ups versus growth those are going to be the two kind of things to think about when you're kind of going through this process here so those are going to be the two like so it really depends on what question you're trying to answer so they both are very valuable when you're doing analytics okay so what we're going to talk about now is this is going to get kind of technical and I hope y'all can get through this so at Facebook they were trying to figure out a way to store historical growth information like because one of the very common questions that are is asked at Facebook is how many monthly active users do we have weekly active users daily active users do we have and one of the things that is annoying about monthly weekly daily active users is especially monthly monthly is annoying because if you think about like if every day you needed to process like okay is this user monthly active then you have to look at the last 30 days of facts right and then do group by on the last 30 days of facts and then every day you have to look at the last 30 days of data and like even though 29 of the 30 days haven't changed only one of the 30 days has changed so like if you think about it in a very like naive approach that would be the very naive approach right is like just every day process 30 days of data and do a group by and see like okay they were active on at least one of the last 30 days I don't know I find that unsatisfactory I think that most of y'all like as a data engineer especially at Facebook if you think about that like yeah there's two billion users and then you think about the fact data there where two billion users each user does 50 rows a day so that means you have that's going to be a hundred billion rows a day of fact data and then if you want to do monthly active you do a hundred billion times 30 so then you're in three trillion three trillion rows so if you did it that way right where you're looking at three trillion rows every day I don't know that sounds expensive right and that sounds like not the most efficient way to do things so they used the cumulative table design just like we talked about in week one we did cumulative table design there and what we do is for cumulative table design we are creating new a new table that we we process 30 days and then the then when the next day comes in we drop off the 31st day and then we keep everything else and you can think of it as like kind of a naive approach to this would be you would have a user ID you have the current date and then you have dates active where dates active is an array of all of the days where that user was active and you can cumul cumulatively add to this array so every time you see a value you can add like another value to the array so that you can see like oh this user was active on this day this user was active on this day data data one of the things that sucks about that though is then you have this big old array of dates and like you don't really need them because dates are mostly you can think of them as mostly as like an offset so think about it this way instead like if you had it where you have this thing called a date list int so instead like imagine it this way where we have user 32 January 1st and then this structure here is going to be the number of days like when that user was active so see this one here that means they were active on January 1st right and then you go back right what this is going to be 1 2 3 4 5 6 7 so here they're going to be 7 days back right so this is like December 23rd or 24th or 25th or something like that of the of the previous year they were active and then you can keep going back so every every day in here is or every bit in here is one day back and then you can get kind of a history of all the users and what days they were active and so that can be a very powerful way to see OK this user was active on these days and this can be a very powerful way to store this data because now you can store 30 days of history as an integer data type and if it's an integer data type you get very good compression integers or some of the highest compressing data types that you can work with so that's going to be a big thing that we're going to be working on today in the lab is we're going to be going over how to create this beautiful, awesome data structure called the date list int OK so that is actually the last slide for the presentation today.

Congrats on getting to the end of the day to lecture date lists are crazy right I'm excited for you to check out the lab as well if you're watching this on the platform make sure to skip over to the next link on the platform so you get the credit that you deserve here thank you so much and keep learning.

## Day 2 Lab (building a datelist data type) begins

One of the tables that we have here is we have this, if we say select star from events, you'll see we have this really fancy table here that has sometimes has a user ID, sometimes doesn't have a user ID So you'll see this is like what this is is every, see some of these are like a hacker trying to like get into my website, there's also this is like essentially every network request that goes on, right? So I just kind of want to go over this because this is real, I just want to show you how this works, so see this backslash robots.txt So this is on the host here is www.exacly.com, so let's go there real quick, so if we go www.exacly.com backslash robots.txt, right? And you'll see this gives us a little file here, so this is like a file that's kind of hidden from most of the people on the internet, it's essentially what this is saying is, you see how I say disallow, I'm saying for ever user ID, go wherever you want, I essentially say Google scrape me, scrape everything, take all my data, like you could be more aggressive about how you disallow things here. I just want to show an example of that real quick, so if you go to Facebook and you go robots.txt, you'll notice they are a little bit more like they don't want Google to just scrape everything. You see here Google, so here's like Facebook talking to Google, it says, okay, you can scrape everything except for these web pages, so that's essentially how disallow works, there's also an allow, but what I'm trying to say for the purpose of this lab today is you'll notice we have a bunch of data in here that goes for a bunch of different websites. One of the things I want to show is let's look at max event time and min event time, because I think this will kind of help you understand, okay, so you see this essentially has data from January 1st of this year to April 27th, I need to update this data. Because I actually have more data here, but this was essentially I pulled this data right at the beginning of the last boot camp that was why it's April 27th, but I'm logging this data all the time, so anyways what we want to do with this data, you'll notice we have some other columns in here, so you see how we have this user ID column, that's another great column that we want to look at. So what we want to do is essentially make it so that we can see on, we can, we can accumulate this up and find all the days that different users were active, because you'll see there's all sorts of different users in this, all sorts of different users, right, and so that's essentially what we want to do. So what we want to do first is we want to create a table, we're going to create table here, and we're going to call it users, users cumulated, and this table is going to have user ID, it's going to be integer, well, let's make it a big int, why is it big int instead of integer, because integer actually has a limit, which is 2 billion. And this number is much bigger than 2 billion, this is like, this is like in, I think the quadrillions I want to say it's a very, very large number, but a big int is fine, you get like, I think you get like 50 or 60 zeros with big int, so that's good enough. So then we have dates active, and this is going to be, we're going to call this a date array, and then we have, we have called this current date, which is a date, and oh, does it not like that. Oh yeah, because you don't want to use current date because it's a keyword, right, what's another way to do this, what's called date, and then dates probably fine. And so what's the primary key here, primary key here is going to be user ID, comment date, where date is going to be the, so I'm going to put some comments here, and then I'm going to paste this to y'all right. So dates active here is the list of dates in the past, where the user was active, and this is the current date for the user. So this is going to be our table that we're going to be working with. Let's go ahead and kind of look at what this would look like on a given day. So we saw that everything starts on January 1st, and essentially what I want to do is I want to build something that goes from January 1st to February 1st, or January 1st to January 31st, and we'll go from there. So in this case, what we want to do is we want to create that same thing where we say like with today as, and then we'll call this yesterday as, and then today as, okay, so in this case, obviously there's better ways to do this where you can like use Python to automate a lot of this stuff. But yesterday is going to be where we read from users cumulative, and we can say where date is equal to, and keep in mind we're starting with yesterday, so that's going to be 2022 1231, that'll be the day that we start with because that's yesterday, which is because we're technically starting on the 1st of January. And so that's what we're going to start with, so what I want to do is I want to paste this down here and we're going to call this from events. So in this case, we have a, we should have event time.

Okay, so this is going to be right now if we query this where we change this to 2023 0101, right, I think this query is going to give us nothing back or it's going to yell. Right, because this event time is actually, is there not something else on that table? It honestly is, because event time should be able to be, can we do like a, I think we can cast this as a timestamp, right, cast as timestamp, and then, and then can you just do a date? I think this works, I think because the actual data type, okay, that worked great. So that's what we'll be using to actually work with things. So this will give us all of the things, you just have to cast the, because it's actually a string in the, in the source data. But this is going to give us what the value was today. So we're getting close here. So now what we need is, we need essentially all the users who were active today. And honestly, this is where you can have a, kind of a different, whatever kind of definition of active that you want. There's all sorts of pages here, right, where, I mean, you could also be like, and URL equals login, right, because this will, oh yeah, there's none. I forgot because it's like, that doesn't start until, that doesn't start until later. But so anyways, you could think about like what event counts as active, for simplicity's sake, because we aren't trying to create anything too crazy here, because this is a one hour lab. Obviously, this would be different in like a business situation. But we're going to essentially start with user ID here. And then we're going to say count one. And then in this case, we, we know that this date. So what we want to do here is, we want to essentially put this date in here as well. Right, and we don't actually need to have the count, my bad, we only need the date. And we can say this is as date active. And then in here, we can say group by user ID comma this guy. Right. And so when we run this, you'll see that this is a lot less. Right. See that this is now, we're down to like 85 records. But there is some more things that we want to work on here, like because there's other problems with this data. You'll see like, you see the null here, see how that there was like a null at the bottom there. And the reason why null happens there is because like, I make a problem in my dev environment. And like, I can't infer the user ID based on local host. So we just need another wear clause, just to deal with the data problems here. So because they end user ID is not null. Right. So that we can get rid of this guy because he's going to be annoying later on with a full outer join. Full outer joins are going to really cause this guy to be super annoying. So we now have our today data and we have our users cumulative data. So what we want to do is we essentially want to make it so that we create the each day, each day going forward. Right. So like, this is very like, I'm just letting y'all know this is going to be a very similar approach to what we did in the very first lab with dimensions. So in this case, what we could say like select star from today, T full outer join yesterday, y. And then we want to say on we could say T dot user ID equals y dot user ID. And so if we run this whole query, there we go. And you'll see that like essentially everything for the y is null, which makes sense because we haven't loaded in any data yet. So what we want to do is we want to get things to match this schema here, this kind of user cumulative schema that we are looking for. So in that case, what we want is first is the user ID. So in this case, it's going to be that same sort of coalesce thing, where we say coalesce T dot user ID comma y dot user ID as user ID. And that'll be our first one. That one is that's the easy one. Then let's go ahead and get dates active. Dates active is a little bit trick. I mean, then we have dates active, which I'm going to skip for now. I'm just going to put null as dates active. We'll work on that in a second. And then the last column we have here is date. So in this case, we have, right, we have T dot day active, which kind of works. But T dot day active might not be there, right? Because they might not actually exist yet. So what we actually want to do here is, it's going to be another coalesce here. We're going to say coalesce T dot date active. Why is that not? Okay. Then we want y dot date as date. There's a problem here, though, because y dot date is going to be off. Because this is actually yesterday's date. And we want everything in here to be the same date. So in that case, you actually want to put plus interval one, I think it's like one day. It's like that or one day. I think that's what it is. Postgres syntax is so weird. Let me put interval like that.

Okay. So we'll test this out. But now we see, okay, we have a good date. And it's always that date there, right? Like that so far. It's looking pretty good. We just need to do that date's active column now to really understand how we can get the right ones here. So this column is a little bit funky because we need to be collecting the array of values here. So we can say like case. So what's considered the first case when yesterday's array is null? So we can say case when y dot date's active is null. In that case, we can say then, and this one's easy. We say array. And then we say t dot date active. Done, right? But then we have a when y dot date's active is not null. Or I guess there's just an else here. The else here is going to be a y dot date's active concat. Pretty sure that's concat. And then we have array of t dot date active. So, and then there's an end here. So this is going to give us essentially what we're looking for. But there is one more, wait, there's actually one more thing that we got to look for. Which is actually, we have another when here. And the when here is when dot t dot date active is null. Then y dot date's active. Because we don't want to just keep adding a big old array of nulls.

So let's run this query, kind of show you what I mean. And then I'll paste this to y'all. So, so far it makes sense. Like it's kind of boring. It's exactly what you would expect. Kind of, it's kind of a snooze right now. And that's totally fine.

So, what I want to do is I'm going to paste this to y'all. Because I think that this is, we're getting a lot closer to kind of what we're looking for. And I actually made a mistake here. And we actually do want to concat at the beginning. So that we have, it's a clearer picture. It doesn't matter that much for the date list. Because the way we're going to be generating the date list, it's not going to matter as much. But for, generally speaking, you want the more recent dates to be the lower indexes of the array. And then like you, you essentially pop it in on the front every day. Every day that the new data is active. Instead of popping it in out the end. So, this is essentially it. Then all we need to do here is we need to put an insert into users cumulative at the top here. And then what we want to do is we can run this. We have big int out of range. It's saying that like, but user ID here is definitely numeric. There's a null. No, because the null's not going to be in there because we have a, and is not null right here, dude. So well, like, let me look at the, let's say I was looking at the whole query here and see like what was going on here. Because, or this is, don't meet boys going on. Let's stop some of this.

Okay, this query here. Run this guy away. So, is this outer range? Is this like, because if we, what's, will we can see, right? Is that like too big?

Okay, wow. This is actually outside the range of big int. Well, it looks like that's really strange.

Okay, well, I guess like, how did this lab work then? I guess we're not going to be using an int or a big int here because it's the data is a little bit different or something. There is not, a big int is actually the biggest integer type. So, in this case, that's fine. It doesn't matter that much. So, what I'm going to do is, apparently, we're just going to drop the table and move them to, we'll move them to a text, and text will work fine. And I did not, last time I did this lab, this worked fine though. So, I don't know. Maybe I did like a new thing with Postgres or something. But there's your, if you just drop the table and recreate, that will be it. I'm going to comment all this out so I don't have to keep highlighting everything. So, what we want to do here is, we're going to cast this as a text. Or actually, we don't need to cast it as a text because it's already a text. We need to cast it as a text here. Because it's not a text in the event data. Right? It's numeric, which is super weird that it's numeric. That's like, that seems off.

Okay, so now, okay, now this is working. This is looking a better, or you see how it's off to the left side now because it's a string. But, um, so now we can take this, and we can say insert into users' cumulative. And this is going to give us good data. So, let's just like make sure that like that data is like what we're looking for. Always like to do that before we like start the cumulative process. So, you see, this is looking, this is looking right, where we have our user ID, our dates, and our date. But, um, that is interesting.

Uh, so, now, what we want to do is we want to essentially build this up a little bit. So, we're going to change this to 2023-01-01. We make yesterday, that, right? And then, now we run it again. So now, if we say like, select star from users' cumulative, where date equals, 2023-01-02, just to show you how like some users here are going to have, yeah, see, now this guy's got two values, two values, and then most people are going to have one value, but some people are coming back, right? And so, this is going to be, uh, how we can kind of model the growth of each of our users here. So, this is getting pretty close. So now we're just going to do a little bit of a manual exercise here, of just like, we need, uh, we need 30 days here, so this is going to take a little bit of time, but it's not going to be too crazy. So, this is going to run this query again for, uh, three, and then, four, and then, five, six. So, this is like, the good thing about it doing it this way, this is probably going to be a better way to, like, run this, where you can run it all in one query. But like, one of the things that I'm trying to emphasize with y'all is like, how I do this in a one hour lab, versus how you do this in production as a data engineer, are going to be different, right? And then you have, like, kind of more of this incremental way that you want to build things up, versus how, like, like, those kind of large backfill queries, a lot of the times, are not going to be as performant as you would expect. So, that's kind of the idea here, where we can kind of build this up, and this will give us, kind of access to, our kind of cumulative query that we're looking for, and this is going to be, almost half way there, yeah. And this will essentially be, like, how all this will build up, and then we'll be able to then turn this query into the date list, and then that will, and then y'all will be like, wow, that is very efficient. And then, from there, that's pretty much what we have for the lab today, but we're going to do a lot more, like, kind of showing the different bit maths. So, you'll see, like, in the date list query that we are going to be working with today, we actually use a bit 32, for our, like, date list data type, and that date list data type is going to be, it's interesting, because, like, you do, it's the same thing, like, you know, where y'all were like, is there anything bigger than long? So, long is 64 bits, right? Which is, like, I thought I had everything covered there, but apparently I didn't. And so, you can technically do this for 64. Most of the ones that Facebook only do 32, because, like, you only really need 32, because luckily, there's 32 bits in an integer, and then there's 30 days in a month, right? So, you have, like, just enough bits in an integer to, like, fit it. So, you have, like, one or two extra to, like, do your kind of stuff with, right? And so, okay, almost there. Last one. So, 30 and 31. Okay. So, now, let's look at that real quick. So, I could say select star from users cumulative, where date equals date, 2023-01-31. So, now, this should have, like, a lot of data. Okay. So, then, you'll see, here's people who, like, come back on some days, right? And then, you'll see that, like, we have a lot more users in here now, because this is essentially any user that was active, at least, one time. And a lot of users here are going to be, like, kind of, one and done users. You'll see we have over 500, right? Because I only get 500 in here, in any one go. So, now we have our data, right? We have our data here that's going to work pretty well to kind of create our values here, right? So, now, we have the tricky part of how do we turn this into a date list, like, into that integer value? That this part is going to be really funky, but I think y'all are going to like it. So, if you think about it, we want to think about this, like, going backwards again, right?

So, where the most recent data is first, and then the oldest data is the last bit. So, that actually means, in terms of bits, though, that it's actually the first bit, right? So, it depends on, like, how you're going to be adding these numbers. Is it the bit that represents, like, one or two, or four, or eight, or 16, or you think about all the different powers of two that you can do there? One's like, I want to see, like, I have that in here, right? I have the, there should be a math dot pow in here somewhere. I have the, this is user cumulative, we have dates, dates active, we have bits.

Okay, yeah, because there's that, you essentially do, so it's the most the bit that's the left the most bit, like, is going to be your most recent bit. That's, yeah, that's what I thought. That's, like, glad I got the example here. So, let's go ahead and look at how to do that. So, in this case, we're going to be using, we're going to generate a date list for 30 days. That's going to be the idea here. So, let's think about how to do that. So, one of the things we're going to be using today is we're going to say, I don't know if you all have ever seen, generate series, but let's kind of go over how this works.

So, we're going to generate a series from the second to the 31st, right?

So, you'll see, this is actually a valid sequel right here. We can actually query this. Date to date. You might need explicit types. Generate series does not work with dates. Does it need to be like a string? Can you do, does it work with string then? If not, I have an idea of, like, how to do this.

Okay. And then, if you do, what if you, like, one to 32, or at the 31? Does that give you, does this give you what you want? Perfect. That is so, so silly that that works, but, okay. So, we can, we can essentially do it this way, where we can, like, we can generate a series that way, but, like, I swear there's a way you can do it with dates. Ah, ah, okay. So, it's, yeah, this, I'm just getting the syntax all weird. So, let's go ahead and, um, do it with dates, because dates are going to be better. So, 2023-01-01, to date. And we have 2023-01-31. We can, so this is a great example of, like, do you use 31 bits, or what's a month, right? So, at Facebook, a lot of the times, like, we actually considered a month, 28 days, because of the fact that, then you have the same, if you have, a month is 28 days, you have the same number of Mondays, Tuesdays, Wednesdays, and Thursdays in that month, and you actually kind of, get rid of the seasonality that way.

So, this generate series works, right? Great. So, this is the generate series that we're going to be working with today, to, kind of, work with our data. So, now, I'm going to just, one of, what I'm going to do is, I'm going to actually just open a new tab. So, let's get this, so I can keep all this stuff, as well. I want to, grab these three lines.

Okay. So, we have our users' cumulative table, right? So, I'm going to say that, we're going to say, like, today's, we're going to say, what's called these users, we call it users, right? And this will just be our filtered table here, that we're going to start with. Then we want our, we'll call this, maybe, series. I'm going to do this, like, with a lot of CTEs, and there's probably another way to do this, that might be cleaner, right? And then, in here, right, this, is going to be, so if we say, like, select star from series, what is this called? I think we want, like, a, okay, it's called, can we put, like, a, as, date? Does that actually work? If you put it there? It does work. Okay. So this is going to be our, we'll call this series date. I think that's a better name. Okay. So now we have our series date, and we have our user's cumulative. So these two tables together are going to essentially, we're going to need to join these tables. One of the things about this is that this join is, kind of an interesting one, because you have, so we know that date here is going to be fixed. So we also need to figure out, like, what the date diff between two days is, so that we can get days sense. So first off, let's do, select star from, from users, and then what we want to do is, we're going to say, we can just say, we can just say cross join series. So let's just look at, like, what this looks like. So we should have a lot of these series dates, right? You're going to have, so let's put it down to one user here, so as user ID equals that user ID. So you'll see what we get here is, for this user ID, we have all the dates that we're looking for. This is exactly what we're looking for. So now what we need, is a way to see, if the series date is in the active array. And if it is, then we create a bit value. Like, probably isn't super, so let's just look at it this way. So I'm pretty sure it's, this is going to be, so if we do a comma star, I think it's this.

Oh, is it the other way? I think it is a date's active. It's like that.

Oh, this is timestamp. Oh, we got to put this as, we got to put this as a freaking date. Operator does not exist date, date array.

Okay, it is the other way around. I got the data types wrong, right?

Okay, what is it?

Oh, you have to compare arrays like this. It's like a weird array thing, right? Where you have like, you have to wrap the other one in array. I remember when I was doing this in the last time I did this presentation. Like, this is, so instead of valid date here, we have series date, which I think is a better name. And apparently I got this wrong. Like, so now you'll see with this user, you'll see how, okay, so you see how he has like the January 30th as a date, right?

You see this column here is now checked. Great. That is exactly what we are looking for. So this is going to be, this is great. We now, we essentially want to put a case here. So we want to say case when we have this, right? Then what we want to do is we want to know the number of days between two dates. I think that works where we can just say date minus date series date, series date. I want to make sure that this actually works because this was, getting back to like, okay, cool, this does. This works exactly what we want. So this is the number of days between the current date, which is filtered down to the 31st and the series date, which is generated from this generated query here. So we're going to use this and math to essentially create an integer here. So in this case, what we're going to say is, so we have that case when statement here. So if they're active on that date, then what we have is a pow, right? And then you have two, two that power, but then it's 32 minus that, right? So that, or, yeah, we'll put this in like, per lens like that. Else, zero, and as int, we'll call this like placeholder int value, and then a comma. So let's just look at like what this does. So you'll see like, okay, for the 30th, right? They get the 30th value, but they don't get the 31st. So this is going to be how we can work with creating our date list int for our user here. So we're really close here. This is going to be, we're going to call this, we'll call this placeholder int. Int, int, as. So, because one of the things that like I want to kind of show here, is we're going to say select start from placeholder ints, and I will definitely get you guys this query in just one second. So if we cast this as bit 32, I think this will make more sense. Cannot cast type double as, well, can you just cast this as int?

Yep, the cast, this is integer as well. Integer out of range of this big int.

Okay, well, there we go. So one of the things I want to show here is, you'll see when you have all of these values, when you cast this to a bit int, you'll see how like most of these are going to be like just a bunch of zeros. Except for one of the days where they are active, you see there's like that one one. And so that means they were active, what that's one, two, three, four, five, six, seven, eight, nine, nine days ago. So this will be nine days ago that they were active. And then you can kind of work with these bit masks to kind of figure out. But like we want the whole history, because you can see like they were active like many days in a row right here, but we need to like essentially sum these integers up. And that's how we can get back to this. So we're not going to cast this as a bit 32 quite yet. So what we want to do is we're going to say, we're going to put user ID here. And then we're going to put some placeholder int value. We're going to say group by user ID. So this sum record does not exist. Oh, it's because it's place, oh, my bad, it's placeholder int value like that. And then let me just show you like what's going on at this point.

Okay. So we have a user and they have this funky sum value, right? And it's like, what does this sum mean? But if we take this and we say, we're going to cast as a bit 32. Oh yeah, this is cast as big int. Like, why does it like, it really wants me to do this twice.

Okay. So you'll see for this user, now we have kind of our history here, right? So you'll see if I like remove this filter here. Now we can get the history of all of the users. Like, what days they were active. Like, see this guy was active two days. And then you can kind of see like, oh, this person came back a lot. And so let me explain like what's going on here because obviously this code here is probably the, like what is going on? So, okay. So let's literally step through this like line by line by line, right?

So in our user cumulative, we have a list of all of the dates where a user is active, right? So if we query this, we have this big list of dates that they're active. So this person was active on three dates, right? So what we can do is if we use, if what this is doing is we're using powers of two, right? So if you think about it where like if they're active today, then we get to add two to the, so if they're active today, that means that date minus date series is going to be zero. So what that means is this is going to resolve to pow two 32, which is going to be two to the 32nd power. And so what this does is this is a hack, right? And what it does is it actually converts all of those dates into your values that are all powers of two. And that's why like before like if you cast this, right? And like you saw how like we also have, okay, so let's just look at both of these together. So you'll see like for some of these users, like especially the ones that have like, okay, so you see this guy has got 16 here, right? So it's because he was active like a really long time ago and you see that like that 16, or you see how he was active. So he was active like very, very long time ago, right? He was active like 27 days ago, 28 days ago. And so you have the whole history. But you'll see that like the actual integer value here, like especially for users who were active on one day, is just a power of two. And the reason why this is useful is if you cast a power of two as bits and you turn it into binary and you actually go into like the binary code, right? Then the power of two actually pops out at you, right? And you see how like, oh wow, this power of two is now like a history. So if you sum these up, that's why if you sum them up, like you actually can get like the whole history of the ones in the zeros. And that can be a really great way to understand like what is going on with this user. So this is a very powerful way of describing each user because now you see how like each user has just an integer value and this can give you how many days they were active in the last 30 days.

Cool, all right everyone. Let's kind of go through how we can look at this and see if someone is monthly active, right? So what we're going to do is, Postgres has a cool function here called bit count. And for some reason it doesn't like this, right? So this query does run now.

Okay, cool. So one of the things you'll see is, in this case, we have all these users and they're all running their data. And you'll see like this bit count here is really powerful because essentially what this is showing is how many days, and you'll see some of these users actually do have zero, right? You saw how there are users here, because they were maybe active on the first and then they never showed up again. And so you'll see how some of these users have one value and then you have people who are really high up on the list, right? Who have like, see this person is active for 21 of the last 30 days. So one of the things you can do is, in this case, you can say bit count and then you say greater than zero as dim is monthly active. So that is a great way to see if a user is monthly active or not, is you can just do that. That is, and then that will give you the number of monthly active users because you just know, at least one bit is on. And so that's pretty powerful. I feel like this function's really powerful. But what if you want to do other ones, right? Like say in, like you have other ones like where you don't want to look at the whole string of bits, but you want to look at a segment of them.

Like another very common use case that comes up at Facebook is weekly active, right? So I'm going to show you how to do that. And this one is a little bit more dicey and not as elegant as bit count. Bit count is so beautiful. But let's go with this one. So what I want to do is I'm going to do cast here. And we're going to do bit 32. So this, what we want to do here is, we want to put the first, well that's 1, 2, 3, 4, 5, 6, 7. So we want to put the first seven bits here as flipped, right? Because that will mean that they, like, so then what you can do is it's really cool. So what we want to do is, we want to take this cast here, and that has the other bit count in it, right? So if we have this, and we cast this as bit 32, there's actually just a single, you'll see there's like a single ampersand. And let's go ahead and look at what this does, because I think that that's probably kind of cool to see. You'll see that, so this is that, so what this does is, this is called bitwise and. So essentially, what this does is, for the first seven, so how and works, right? If you have an AND gate in electrical engineering, if you have a 1 and a 1, then you get a 1 on the output. But if you have a 1 and a 0, you get a 0. Or if you have a 0 and a 1, you get a 0. Or if they're both 0, you get 0. So what this is doing is, we're throwing these bits into an AND gate, and everything that's after the first seven days is 0. So because we don't care if they were active eight days ago, for weekly active, we don't care, we only care if they were active in the last seven days. So what we can do here is we can use this thing called bitwise AND. And so this is going to give us, so what you can do with this is you can do bit count on this whole thing. And then you can say greater than 0 as dim is weekly active. And that will give you, you'll see, now you can see that, like obviously the people who are monthly active or the people who are weekly active are going to be monthly active, but not the other way around. Someone could have been active like two weeks ago, and then like this guy here is active a while ago, but not active this week. So you can do these cool like bitwise operated functions that can kind of take this dateless structure, and then you can see like, oh, is this going to be the right values for us, right? And obviously you can do daily active here as well, like where you just essentially copy this guy again, and you just instead of putting seven here, right? You put just the one, and then this is dim is daily active. And this will give you all three, where you can see like, okay, are they daily active, right? And you can see all the people who were daily active, weekly active, and monthly active on that given day. And that is like essentially how you can very quickly use this structure, because again, this AND here, and this bitwise AND is also an extremely efficient operation. But computers are meant to do this. Like when they say like computers work with ones and zeros, like that's literally what we're doing here, is like we're working with binary, and what we can use is like we can work with binary, and then we can get our kind of business questions answered from the binary. And so that is kind of what this does, and you can build really powerful things. Obviously, you have to teach your analysts how to do this stuff. But like, is this that much harder than teaching them how to work with arrays, like and do stuff like this, right? It's about the same. And like, but if you can get people on the same page on how to generate these, or like don't, right? And what you do is you just put that behind the scenes as a data engineer, and then you give them the monthly, weekly, daily active flags for that day, and then they can work with that, right? And they don't have to work with the history, they don't have to work with the bits, because like honestly data analysts aren't going to want to do that anyways, you know?

Congrats on getting to the end of the day two lab. Those bit functions are freaking nuts, right? I'm excited for you to check out the day three lecture in lab where we do reduced facts. If you're watching this on the platform, make sure to skip over to the next link, so you get the credit that you deserve.

Yeah, thanks so much.

## Day 3 Lecture (reducing shuffle with reduced facts) begins

a specific chunk of data to all be on one machine, you can have it where all of it split out and we're gonna talk more about what that means and the parallelism of stuff in another slide. But keeping in mind that some steps in your big data pipeline are gonna have more parallelism than other steps in your big data pipeline. And because of that, usually the more parallelism that we can have, the more effectively we can crunch big data because that is gonna be what makes it possible for us to use more machines and to have less bottlenecks because shuffle, you can think of shuffle as the bottleneck. It almost always is the bottleneck for if you were working with very high volume data. So there's a couple different ways that we can think about minimizing shuffle but we'll be going into that. There's like a modeling way to do it. There's like also like more kind of tactical ways to do it that we'll talk about. So yeah, let's talk more about shuffle. Shuffleing is so fun.

When you are working with big data, there is, you have in SQL, right? You have a couple different keywords. Select from where group by join having order by. These keywords affect shuffle in different ways. For example, if you have a query that is just select, that just uses select from and where and importantly, your select doesn't use a window function but like, so you have select from and where without a window function. That is what is considered a query that is infinitely scalable. You could essentially have it where like, if you think about it this way, like say you had a billion, like let's think about it in like kind of the most extreme sense here, right? So say you had a billion rows and you had a billion machines and each machine had one row. The thing is is like with select from and where, that's fine, right? Because one machine doesn't need to have more data or less data because all it needs to know is like, okay, here's the data I have. Does this data fit this condition or not? And if it does, okay, bring it out. Otherwise, don't bring it out and that's it. That's all like you don't have to have like a specific chunk of data on a specific machine. That's the big thing to remember about select from and where is that like if you're using those three, like you can go as parallel as you want and that is very, very powerful if you can understand that fact because you don't, you can use, if you have queries with those three keywords but only those three keywords, like the job is gonna essentially come back instantly. The job is gonna be no shuffle. The job's gonna be cheap. The job's gonna be all those things, right? So now let's talk about the ones in the middle. Here we have group by join and having. So why are these not extremely parallel? Well, the main reason for that is because, so say you have group by, right? And let's go back to that example where we have a billion rows and a billion machines, right? And there's one row on each machine. Well, with group by, the problem is, is like in order to actually do the right aggregation, all of the rows for that key, so say there's me and I'm someone who has 30 rows in this billion row data set, right? And all 30 of my rows need to be on one machine. They all need to be moved over onto one machine in order to correctly count them, right? Because if they're all separated and my 30 rows are all on different machines, then we, how do we know that there's 30? We don't know, like that's like, and that whole process of collecting, so the data's in a bunch of chunks and it's all separated out into like these, this comically chunked out like one row per machine kind of situation. And in that case, we'd have to move all that all those into one machine where then it has all 30 rows of data so that it can count at that point. And that's why group by triggers shuffle. And that's what all shuffle is, right? Is shuffle happens when you need to have all the data from a specific key on a specific machine. That's all it is. And so, and that's the big thing to remember when you're doing shuffling. And so that's why group by works in this case, right? Then you would reshuffle and then, and essentially what it does is you get a pick, you get a pick the parallelism there, right?

Cause in Spark, there's a thing called Spark.sql.shuffle.partitions, which is a configuration that you can pick, which is something we'll talk about in week five as well. But the default is 200. So we would have our billion machines, they would pass all of their data to 200 machines based on the kind of the key, right? So say you have user ID and the user ID is an integer. And then what you would do is you would take them, like the modulus, so you would take the, you would divide the user ID by 200 and whatever the remainder is, that's the machine it goes to, right? So say it's like user ID is 207. So that would go to the seventh machine, right? And user ID 402 would go to the second machine, right, et cetera, et cetera. And that's like how they kind of like split it out. And you can change that number, that 200. You can change it to whatever you want. 200 is just the default. But anyways, Group by is going to do that. It's going to kind of bring everything back together and it will do things where you have those 200 kind of machines that will process and then aggregate the data once it's all collected along whatever you're grouping on. Then it will then do it that way. What about join? Join is actually even trickier in some regards because join, you have to do this not once, but twice. So what you do in the join case is you have all of the keys on the left side and all of the keys on the right side and then the keys on the left and the keys on the right all have to be pushed to one machine and that's going to be one partition in your shuffle partitions. Which again, defaults to 200. But then after that, then the comparison happens to see like, oh, do these match or not. And then if they do match, it keeps that record and if they don't match, it chucks it away or maybe not if it's a left join or a right join or whatever depends on the join. I'm talking about inner join in this case. Then having, so having, I was having difficulty, having difficulty putting it into a bucket here, I put it in kind of parallel because of the fact that you have to have having a group by kind of go hand in hand. So technically having is like as parallel as select from and where because it's just again a filter condition, but it's a step after group by. So you can only really apply having after shuffle. So that can be a big thing, right? And then let's talk about the last one here in distributed compute, which you should almost never use, is order by, order by is the most painful and least parallelizable keyword in SQL. And here's why. So if you think about order by, especially like, I'm talking about order by at the end of the query, not order by in a window function. Order by in a window function is a little different and we'll talk about that in a second. But if you have order by at the end of a query, what ends up happening? So say you order by user ID and you need to do a global sort of all the data. And it's like on, and say it's on 200 machines or it's on a billion, we'll go back to the comical example where we have one row of data on a billion machines. And then we need to make sure that all the data and all those systems is sorted. The only way that we can make sure that that is the case is if all of the data goes on one machine and it all gets passed through one machine. And so which is, I don't know if you'll all know, but that's like the opposite of parallel, right? Cause that's one machine that's sequential at that point. And that is where a lot of times you'll see this can be a very big symptom in big data jobs if you have a order by like you can use order by but use it at the end after you've like aggregated and the number of rows that is left is like in the thousands or tens of thousands. If you're using order by and there's millions of records like your job's gonna have a bad time. It's gonna have a really bad time. So how is this different in a window function and why is a window function not gonna be like if you use order by in a window function? The main reason for that is a window function doesn't do a global sort. But it can, it can, right? If you don't put any partition by in there and if you're doing like a global rank function so if you use like rank and you don't partition by anywhere in the, if you don't use the partition by keyword then you're gonna have the same nasty, terrible parallelism problem.

Exact same problem like so like but if you're using partition by which you should then it's no longer a global problem and you don't have, then it goes back to the shuffle problem where you end up sorting based on the number of shuffle partitions which is usually 200 but you can change that if you want and because that partition by is going to create that same sort of thing where like we need to cut up the data and pass all of that same data to one machine and that's the, so partition by is in that way it's really weird because partition by in a window function and group by in like a regular query like in the distributed compute world those things are, those things behave very, very similarly so especially in terms of how it affects shuffle. So that's where order by is an interesting one like you can use order by in window functions only if you also have partition by like if you do it by itself like you're gonna have a bad time. So why am I talking about this?

Like some of this you're probably like exact like I thought we were talking about fact data modeling like why are it, well this doesn't seem like anything that's related to fact data modeling at all I'm just like I'm going off on this rant about spark and distributed compute and one of the things though that I think is very important to remember about this stuff is that how the data is structured determines which of these keywords you have to use and if you have data that's structured in a certain format you can skip out on using some of these keywords. So that's a big thing that you wanna think about when you're like kind of going through and building out your fact data modeling is that. So remember this, at the very least just remember this slide is very powerful. It has more of an impact besides just on data modeling exercises it can help you troubleshoot spark problems and other problems as well. So it's great. So let's go to the next slide.

Okay, so group by right we talked about how group by causes shuffle. So how can you fix that problem? So there's two ways that you can fix this problem and here's essentially how it works. You can in spark or in not just spark, but like with S3 or any kind of like or iceberg or hoodie or Delta Lake or whatever they all support this is you can bucket your data. So instead what you can say is like you can essentially do this you can pre shuffle the data for spark. So like say like you're like, okay, I wanna put my data into eight buckets and then when you and then you bucket based on a key probably it's gonna be like a user ID or a device ID. Usually it's some sort of like high cardinality into your field and then you bucket on that field and then you and then it does that whole modulus grouping when you write the data out. So if we have the modules grouping when it writes the data out then when we do group by we don't have to shuffle because it's already in the buckets are already set and we already have the guarantees that this data is here and this data is here. And so then the group I can just leverage the buckets and it can just crunch it down and that can be very powerful. That can be a very powerful way to do group by without shuffle. And so definitely we're gonna talk more about that in the Spark week, week five and that's one way to do it. But today we're gonna be talking more about the other way which is all in caps here which is reduce the data volume as much as you can and like because if you can reduce the data volume and you can also reduce that that also reduces shuffle a lot and you can really create way more efficient queries. How can we represent fact data in a way that really is efficient and wonderful and can do really magical things. So let's get into a little bit reduced fact data. So we talked about fact data before. Fact data often has a schema where you have like a user and then you have a time and action and a date, right? And this data is very, very high volume. Incredibly high volume, then you have the next layer down is you can aggregate this up, right? So instead you could have like a user ID and then you can have like an action count. So in this case you could say like I made seven likes on January 3rd, right? And that could be that daily aggregate. That is great, the volume is dramatically reduced that way, right? It's like it gets cut by the number of actions per user. So a lot of times that might be 100x smaller. That's good. But even that data can be too big and I'll talk more about like kind of the problematic things that can happen there. And that's where we're gonna go one step further here where we talk about this reduced fact, take this one more step, right? So in this case what you have is you have a user ID and then you have an array of action counts. So you can think of it at this way. It essentially cuts the daily data to it divides it by 30 if you use month. But it can also divide it by 365 if you use year. So it's like you can pick which way you wanna do it. But this data ends up being like you can either have one row per month per user or one row per year per user. And those are a very powerful way to get the data essentially as small as you possibly can get the data. And that is what can really minimize the amount of shuffling that can occur and really minimize the whole thing. Like it's really powerful. And we're gonna talk more about like how this unlocks a lot of new capabilities. But the key thing to remember here is you got three flavors here. You have like just normal fact data, very high volume, daily aggregates, medium volume. And then you have reduced facts which are the lowest volume. They come with some trade-offs though, especially between the second two, like between daily aggregate and reduced fact. There's some interesting trade-offs that we're gonna kind of dive into as well. Because as the data gets smaller, you lose some of the flexibility around what type of analytics you can do on it. But that's like worth the trade-off of being able to do analytics more quickly. So that's one of the things that you'll find in data engineering a lot of the time is that when you are working with data and you aggregate it up, the data gets smaller, easier to work with in terms of like speed of queries. But then it's like you're like, oh, what if I wanna do this or what if I wanna do that? And then like some of the options are taken away from you because the data is too aggregated up. So let's go a little bit deeper into each one of these.

Okay, so this is what a fact schema might look like where you have like a user ID, you have a timestamp, you have the action that they took, you have the date, the date partition. And then there might be like other properties. Like for example here, it's like I made a like on this post on an Android phone at this time. And then here's like I commented on this post on an iPhone at this time. And so like the thing about fact data, like when you're here it's very, very granular and you can ask very, very specific questions of like whatever question under the sun that you wanna ask. But the trade-off is like you can only ask questions usually for like what has happened recently or like within a couple of days, you can look at very small time horizons. And the reason for that is because you end up missing like the volume ends up being unwieldy if you have two, if your time frame gets too big. So you can't like the fact data, especially if you're looking at more than a month of fact data like if it's high volume enough then like a month of fact data is gonna cause like all sorts of mayhem and all sorts of problems. And it's gonna cause like, it's just gonna cause your queries to be really slow and like they might not even run. You might get some weird out of memory problems. Like there's all sorts of interesting things that can happen when you do your fact data modeling and or your analytics just right on top of fact data, right? But at the same time like this is great like for other questions, right? If you're answering very specific questions like what happened today or what happened in a week, what happened in two weeks, right?

One of the things that I noticed at Facebook is like, so they had this thing where like after 90 days everything had to be anonymized. So like that policy also really impacted analytics a lot whereas like it made it so everyone was just looking at the last 90 days of data and like they didn't like look back further and they weren't trying to figure out like, okay, how do we get like the other days of data in here as well? But anyways, this is a crazy problem. That can happen, but so fact data, the key thing I'm trying to take away from this slide is that this schema is very flexible, very good at answering very specific questions, but it's very not good at answering questions over a longer time horizon. Once you have a longer time horizon, you're gonna have a hard time using this schema, I promise.

Okay, so let's go to the next schema, that kind of that daily aggregate, right? That was the next one that we were talking about. There used daily aggregate. So daily aggregate, you have a couple things here, right? You have, again, you have user ID, then you have like a metric name, in this case, likes given, you have the date and then this is how many metrics or how many likes were given. So now we have our daily aggregates. This is gonna look a little bit nicer. Now in this case, you have one row per user, per metric per day. This is a little bit nicer instead of having like, you know, for likes, like, for example, this first row here, there was 34 records in the old table and now there's one. And so that is a big difference, right? That's a, that aggregation is gonna make this table a lot easier to work with and you can do a lot of really powerful things with this table as well. So one of the things that's really nice about these daily aggregate table is you get a lot longer time horizons now. Like I noticed, at least when I was working in big tech, that like if you have this schema, this works for like, if you wanna look at like a year or two years, this works great. This works great and this like can work really well and the thing that's really nice about it is you get these nice joins because say you have like an SCD, right? So you wanted to know like what my primary device operating system was like, whether I was on Android or iPhone and like which was my favorite and that might change over time. But the thing that's nice about these aggregates is you have that at least down to the daily granularity. Obviously you don't have like the hours. You lose, you lose a little bit there. Like if you have something that's changing like on an hourly basis, then it's not gonna work. But like most SCDs are on a daily basis anyways. So you can still join this table with SCDs and you can still do like aggregates at the higher level of bringing in other dimensions and that can be a very powerful way to use this schema. So and it works great. Those year long or two year long queries, they do take a little bit of time though. They take like, I mean, at least back then we were working with Hive and those queries would take like two days, right, ish, two days to work with. And that's like kind of a showstopper. It's kind of not the way to go. And you can't analyze dimensions very well if it takes that long. Like obviously two days is not that long but it's like still long enough to be painful. So one of the things about this aggregated data is it's very powerful. So this schema here is daily aggregated facts is how's another name, it's called like a metric repository. Think of it that way. We're like, in this case, your partition on this table is actually not date. Your partition is date and metric name. You have a sub partition on metric name so that you have two partitions for this table so that you can dump in all your metrics into this daily table. And this schema is very, very powerful. Like so many of the, I don't know if y'all heard of Deltoid. Deltoid is an experimentation framework at Facebook that powers a lot of like the A, B testing and all sorts of different powerful things. It uses this schema to determine if an A, B test impacts different sort of metric values and all that kind of stuff. So one of the things I noticed when I was working at Facebook though, was this schema could still be improved on. We can still make this smaller and not lose anything. There's still one more layer we can do. So this schema is what was available very powerful when I was working at Facebook. So this schema probably looks a little bit crazy but we are gonna be working with this schema today and we're gonna be building out literally this exact data model in our lab today. But let's kind of go over like how this works, right? So you have a user ID, you have likes given. And then in this case, there's only one row per month, right? And then you have your value array. So one of the things that I recognized when I was working on this problem because a lot of this stuff like why I even had to come up with this model, right?

Was I was working on these problems of like there was the first decline in growth at Facebook in the history of the company and they were like what's going on, what's going on?

We need to look at all the metrics. We need to look at all the metrics over the whole period of Facebook, how do we figure out what the hell's going on here and how do we can troubleshoot this, right? And that's where we were mostly working with these daily metrics and I was like wow, these daily metrics are pretty fast but we had that two day, three day backfill problem and I was like that's not gonna work, right? That's not gonna work. So then we moved to these monthly metrics, right? Which essentially reduces the volume again, 30X because of the fact that now we and but the thing is is like keep in mind, this is not a monthly aggregate. That's the thing that I hope y'all keep in mind here is that's why this is a beautiful thing. If it was a monthly aggregate, this would be boring and like y'all would like not like this and this would be kind of stupid but this is actually not a monthly aggregate. So how this works, right, is we have this month start and then we have this array of values. So the date is an index. So in this case, 34 is how many likes user three gave on July 1st and then three is July 2nd, three, July 3rd and then nine is July 31st. So what this does is instead of storing the date as a string, right? In multiple rows, we store the date as an offset or as an index. So you do date math based on the index. The position of the position in the array is equal to what the date was. So this is very similar. Like in a lot of ways, this is similar to yesterday's lab on date lists, like if y'all remember like how we have that craze like bits of ones and zeros and the position based on like where if that bit was the first bit that meant it was for today and then the last bit meant it was like 30 days ago or whatever. This is a similar concept except you are doing it for non-binary things, right? You're doing it for things that are either like a decimal number or like a count can image your value.

I talked about a lot of this stuff already where I said like the first index is for the date at the beginning of the month and the last index is for the date at the end of the month, right? And I was talking about how dimensional joins get weird if you want things to stay performant because you don't want to like have to bring in a dimension in the middle of the month. That was actually one of the things that I was trying to figure out. Like that was like one of the last things I was looking at Facebook was like how to do that? Like how to actually make it so that the dimension gets assigned to the right value based on the date and like based on the index and the array and then the sum happens the right way without exploding it and I couldn't figure it out. That was like I was like it's too crazy, too crazy and I was like I just and that's one of the beautiful things about data is that like if you explain to people the constraints of what is what are the oddities and let it the weirdness of things that can happen with this and then they can accept that as like okay that is inconsistency but like it's fine for like what I'm trying for the purposes of what I'm trying to use this for. Like it's great, right? And so that can be a really powerful thing that will work, right?

So what this did was we were able to enable long term analyses that were a lot faster. So because the next slide. So here's the impact from this new schema, right? So before if we wanted to do a 10 year backfill with that intermediate layer, that daily data, a 10 year backfill would take about a week, a week to maybe a little bit more and once you got the data into this format instead of taking a week it took a couple hours. It took like three or four hours to run because it's just dramatically smaller, dramatically smaller data and it made it so we could cut up the data and look at all every possible dimension under the sun.

Like for the entire history of Facebook as quick as we wanted. And that's like, I always talk about this like unlock the decades long slow burn analyses at Facebook which is like okay, is there a certain type of user that is not doing a certain type of thing over a long period of time but the decline is so slow that our alerts don't pick it up and our alerts don't see it, right? So then what we're able to do is just like kind of look at all that stuff and like, I don't know if y'all have ever heard of like root cause analysis or RCA. So RCA is a very powerful thing to try to troubleshoot what was going wrong and what happened, right? And that is a big thing that this framework unlocked. And so yeah, this is very powerful. I felt very proud of building this out in a lot of people in the analytics space at Facebook where like wow Zach, you're actually doing stuff that is kind of innovative and new things here that are kind of changing the company. And so this is really powerful and I think I have one more slide here. So I loaded this framework up with 50 core metrics and 15 core dimensions and this enabled 10 year analyses that happen in hours instead of weeks and they didn't promote me. They said that was not enough of an impact to be a senior engineer. And I was like very angry about that because I thought that that was, because I thought I solved some really insane problems with this framework. But yeah, that was, so the fact that Facebook did not recognize the impact of this. But it's crazy because then later on they did because like a couple of my teammates who stayed, they kept working on this project, right? And they got this thing called discretional equity which is where Facebook will give people like another big equity grant to keep them and maintain them. And so they made millions of dollars off of all my hard work. And they're grateful for it. I'm still good friends with these people. But I'm always like, I tell them sometimes, I'm like, y'all owe me, y'all owe me money dude. Like so yeah, I think that is what we got for the lecture today. Congrats on getting to the end of the day three lecture. Reduced facts are crazy, right? I'm excited for you to check out the lab and how to minimize shuffle and get in there and get hands on. If you're in the platform, make sure to skip to the next link so that you get the credit that you deserve.

## Day 3 Lab begins (building reduced facts) begins

Let's call it a ray matrix. Then we have a user ID. Apparently we want to call it numeric because numeric will work.

I remember in yesterday's class it was like weird. We'll just match it with whatever's in eventals. Even though I don't like the word numeric, I don't even know what that means. It's fine because we used big and last time it didn't work but I think numeric will work. I don't want to use text. We have user ID. Then we have a month start. We'll call this a date. Then we have metric name. Metric name is going to be a text. Then we need the array. I'm going to call it metric array.

There's a big debate here about what type should this be? Is this a real array? Or is this an integer array? Or is this a scoring class array?

If you want to do array of struct, I'm not about that life. I think we're going to do integer array here. You can say integer or real because if you use real, you can put an integer in a real but you can't put a real in an integer.

OK, fine. You guys convince me or I convince myself we're going to use real. In this case, we have a primary key here. Primary key is going to be user ID month start metric name. This is going to be our table. We're going to be working with today that we will be building up slowly but surely. So let's go ahead and create this babla. Cool. So one of the things about this that is kind of tricky is that you have to think about this in terms of partitions and like hive or like a partitions and things like that. And that's where this like could be a little bit messy compared to like in Postgres, whereas it's a lot cleaner. Like when you have like that insert over right sort of mentality, right, because in this case, we need to have month start, but it'll make more sense like I will cross that bridge when we get there, but I'll show you what I mean. So what we want to start with here is we want to create that daily aggregate function. That's actually not too hard. So let's go ahead and say with daily aggregate as so we're going to be pulling from events here. I'm going to comment this out pull from events are going to say select star from events. And then well, what do we want from events?

Well, we want a user ID and then we're going to count one as num site hits. And then we're going to have a where here and then we should be able to do date of event time equals date, let me say like 2023, 01, 01. So we're just going to do we're going to do like the month of January. And that's what how we're going to build up this array. So if we do right now and we say daily aggregate, we're missing though, we're missing, we're missing the group by. So we've got this guy to run him. Cool. See, we have all of our hits.

Okay, that's a problem. We need to get rid of that. This null. So and I'm going to paste this to y'all like because obviously I just like jumped immediately into coding. All right. I was wondering how you were typing so fast while speaking. So good. Okay. So we have our aggregate for the day. Right. So like what we want to do is we need like yesterday's aggregate. And so this is where like, like for the purpose of this lab, I would change this because you you don't get this is one of the things I hate about post. So one of the things I hate about Postgres that I wish that Postgres had was like a merge because Postgres doesn't have merge right.

Yeah, it doesn't have. I think it has like, I think you can do on conflict on conflict update though, right. I think you get that. I think we can use on conflict update. We'll try all on. I might end up googling a little bit here. But so in this case, we have our daily aggregate. We need a last month's aggregate as well. Like we need like, we need like what was yesterday, because if you think about this in production, when this is running on January 1st, the array will have one value. Then it will have two values. Then we'll have three values. So that's something that we need to consider when we are building this out. So let's go and get like yesterday array as. And then in this case, we're just going to say select star from array metrics, where in this case, we're going to say month start equals date 2023, oh, one, oh, one. Because that's all we care about. So now in this case, we can do like a of our daily aggregate full outer join yesterday array. One of the things that I hope y'all like at the end of this data modeling stuff is that like you'll recognize that every data modeling problem is actually the same problem where you just where you full outer join all the time. And obviously like if I said the unlinked in people to be like, Zach, did you have a stroke? But anyways, let's just run this now. And I think this will make more sense. So we have our site hits and then we have noles across the board on the other side, like we would expect. Right. So now what we want to do is we have all this then what we want to do is we want to essentially create an array or we want to create a new array for this daily aggregate if it doesn't exist. And then we have to fill backwards like essentially from that date. So that's where we actually do need to pull in this date here. So because this is going to be needed to say as current date. Because we need this, oh, we'll call this fund was called as date. I hate how postgres doesn't like current dates like a keyword. So we're going to need this and we're also going to need to group on it. Right. Because we'll need this to do the date math later for like there's an edge case where we need that I promise. So initially what we want right is we're going to have a coalesce here of DA user ID and YA user ID. This is always the best part when you just like, yeah, I got my coalesce figured out. And then now like, okay. So then month start, right month start can just be like another coalesce here. So the coalesce here is going to be between and this coalesce is really, really gnarly actually. So in this case, we have you have YA dot month start comma. And then you have DA dot date, but as a month start. But this is not quite right because this day moves forward. So you got to like truncate this right. So you got to do like date trunk month. DA dot date. So that like as we kind of accumulate up, this will still stay month start.

Okay. So now we have month start. Then we have metric name. Good thing. Good old metric name here is called a site hits as metric name. This is something that like is usually hard coded, but it's good. Now we have the hard part building the damn array. So you can think about it in this first case when yesterday array is completely null and completely empty. It's like pretty straight forward because we know all the users are on the other side. So are there only in the daily aggregate, but they're not in the yesterday array. So we're going to essentially fill in the first one. And because I want to I want to really illustrate to y'all the pitfalls here that can happen. So in this case, we just want to do array. And then so we want to say case when YA dot metric array is not null. Then what we want to do is. If the metric array is not null, I want to say y dot metric array. But we want to. So this is the next day though. So this is the opposite. From yesterday's date list one where we put the most recent data first. This is actually the other way around because we want everything to line up right. That's one of the things we want to do here is we want to have everything line up. So what we want to do is we want to do a concat here. And then we want array. And then we have DA dot num site hits. But there is this is another one of those edges where this could be null. And that might be okay. Y'all might be okay with null. Like I think for this case, I don't like null. I want to do zero instead of null. So that will be. So then we have else.

Okay, so then we have a so if the metric array is not null, that means the user already exists. But then we have when YA dot metric array is null, then.

For now, what we're going to do is we're just going to put in the. This value here. But this is actually wrong. And I will explain why this is wrong here in just a second. But you'll see with this kind of daily aggregate. This is getting us pretty close. So you see we have our user ID and we have our case when statement. And it looks really nice. While someone someone hit my website 60 times on New Year's.

Wow. So I'm going to need to get a life. Or maybe they just love my data engineering. Maybe it's just a super fan. I'm sorry.

Okay. So then in this case, this is metric array. I want to say as metric array. This case, we need to put like an insert into here insert into array metrics. Right. And then on conflict. So in this case on conflict. So our primary key here, we got to put all of them here. So we have on conflict. Then we have all of those right. And then we say set and then we say metric array. And then in this case, we say equal to what is this even doing.

Okay, because we want it to be. I think we want it to be the excluded one because this is going to be the. And the other record, right. And so I think this is just dot metric array. I think that's all we do. So we'll see if this actually works. But there is one more bug here with this guy, which I will kind of show here in a second, but we had to get this on conflict right. I'm glad I got the on conflict stuff to work so we don't we can do it the right way. In a big data world, like you don't have to worry about this because you get overwrite, right. And overwrite just will just like you don't have to worry about how to set the updates of things. I'm like, I don't know. Maybe I was like, I've been spoiled and I've just been using overwrite for so long that like I just expect it out of every technology I work with now. And every time I'm like, why do I have to update? I don't like the update keyword. So anyways, let's go ahead and we should be able to run this query now.

Okay. So we ran the query for day for the first day. All I want to do here is I want to run the query for the second day. And just so I can illustrate the problem. And check if this conflict thing works.

Okay, well something works. So if we say select star from array metrics, we should have some here that have two values.

Okay, there we go. Perfect. Yes, everything, everything is exactly what I was thinking was going to happen. Okay, so remember in the okay. So you see how like for some of these metrics, like you see this first guy here is going to add six on January first and zero on January second. So he didn't show up the second day. Right. This person was three and three. Just very consistently going to three pages. Right. And so you'll see though, like remember one of the things I said was that every for every iteration of this for every data set here. Regardless of when a user shows up, everyone should have the same number of elements in the array. In this case, they should have like these guys should have one more. They should have a zero at the front because this person essentially didn't exist until January second. And that's what is going on here. So what we need to do is there is. So in this case, we have the okay, if the metric array is null, then we need to have this array. But there's also it's really awesome. So we have an array fill function. Right. And we need a concat here. So the array fill function here is actually going to be equal to. So in this case, there's a month start. So then we have we have date and minus month start. So this probably looks really funky. But like this is what this is like. So what this does array fill what this is going to do is it's going to. So for the second or what's imagine we're on the seventh of the month and a new user shows up. Then what this will do is date will be the seventh of January and month start will be the first. So then this will be six right you'll have six values that are there that need to be kind of. And so what this will do is this will create an array of six zeros. This will be zero zero zero zero zero six times. So one of the things I want to do real quick is I want to like clear out array metrics though. We're just going to clear them out because like. And it's going to yell at me because it's saying there's no. I love that data group does this so that like because you don't want to delete all the data. But we do want to delete all the data. So this that's what this array fill is going to do. And what we're going to do is we're just going to move this back to January first. And then we're going to run this two times. So we're going to run it for a ray fill integer integer does not exist. So this is so weird. So you actually give it an array like that. That is so weird. But OK. Was that like dimensional values cannot be null. Oh, is it because month start is null. Right. Interesting because that is null. And then this is null because it doesn't exist yet. But oh, this is this is an interesting edge, right. So in that case, we have, I think there's like a third condition here actually. So when it's completely empty, this is not going to work, right. So when it's completely empty, though, we can just have this first array because we know that that date hasn't happened yet. So what we have here is like when y a dot month start is null. And we have that array. OK, I think this should run now. OK, it ran. So if we look, look at it as I like, we search here, this should perfect. So the first day ran that worked great. But then it's moving to the second day so we can just see this working. And then I will definitely send this query to y'all. OK, so that should now we should get our field that did not give us the field zeros. Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, because no, that actually makes sense because, oh, because it's not matching here, right. So that's still going to, yeah, this is wrong. Actually, like, so we got to like essentially coalesce this because one of these values is going to always be there. Right. Because essentially what we want is like if both of these values are the same. So that's so weird. I didn't even like, I thought I ran into this problem before. I know I'm like kind of fumble in here in a second, but like, let me, let me go over what we actually needed to do here. And what's going on. So the problem here is this array can't accept a null value. So what we want to do is we just need to coalesce this to zero. So like if either of these is null, then we just don't fill because we don't need to fill because that means it's the first day of the month. Right. And that will fix our problem. But now we have bad data again. So we had the delete from the array metrics, but that will be good to go here just a second. OK, so that will fix our problem. That's why you have to put a coalesce there because you can't put array bracket null because postgres apparently doesn't like that, which is again, like one of those like today I learned sort of moments. So I think this query should run now. OK, but then if we change this to two, this should run now. OK, now we should be good. How is that still not like OK, now I'm wondering if like the update isn't working if it's something with the update actually that is because here we are getting our the array fill because if you have the new date, oh, I know what it is. I know what it is. It's because month start is still null because what we need to do is this month start is not actually in this yesterday array. This is a hard coded value. This is actually not here because what's happening right now, which is kind of kind of go over what's going on right here. So we have the date here. And we pull it in from the array, but we have a full outer join here, right. And so when I have this month start value here, this is not the right one because this is going to be null. So on the second, if someone shows up and they don't exist yet, this is going to be null, but really in the pipeline, this is not like this value is fixed, right. So this is actually date 202301 and it never changes that date will never change that dates always the same. So like that's why we're still getting buggy data.

Wow, that's a very interesting, a very interesting change.

Okay, there we go. Now, now we'll be good with just like one more delete. And I think we, I think we got it here. That's even better. I love that. I love that.

Yep, but we're going to move it to that. Right. Because they, so you're saying date trunk month of date, right, like that.

Yeah, I like that better. I like that better because then it's not hard coded right because then like you don't have to like if I want to change it to a new month, I only I still need to edit it up here. Right. Right. So, okay. Now, keeping in mind that like this kind of array fill stuff, it's, this should work. I'm okay. We got to change this back to one, though. So you'll see like this has the same pitfalls that accumulation does.

Ooh, types interval and integer cannot match what coalesce date trunk. Because this is because this needs to be cast as a date as well. That's postgres is so weird. Well, because it worked before.

Okay. No, yeah. There we go. It's because date trunk returns a timestamp. That's why. So you got to wrap that in another date, right? Because that's like so dumb.

Okay. Some of this stuff like all this silly little data engineering stuff is.

Okay. Now like my whole point is I just wanted to get it to be where everything in the metrics array. There we go. There we go. I know that was painful. Y'all, but there we go. We got it. So one of the things to prove it out right is we can say cardinality of metric array. And then we say count one. And then we can see like how like everyone should have this should be two. Right. Everyone should be two. Yep. There we go. 138 users. Everyone has two values. Right. And then this just keeps working to though. Like you'll see if we if we go to three. Right. And then everyone will have three values now. Right. If we kind of where did that query go? We'll put that back and then change that to three. But you'll see now if we run this query now everyone has three. So that's kind of the idea here here. Let me paste this to y'all because that was there was that weird date cast that I think we missed that this query essentially does it where we can build these things up and run all these queries at once. And we can get all of the data. Obviously like. This line is this line is this line is absolutely nuts though. I don't know if y'all like if you look at this line of code you're like what is this guy doing here like this is so crazy. But that gives us our code for for that right. So one of the things that I wanted to show though that I think y'all will really appreciate is how to do the aggregation of this so that you can see how we can do that. So we can go we can aggregate and I'm just going to show how to do it with with metric name and we can group on metric name but then it will be obvious how like because you can join on user ID and bring in other dimensions if you want. But you can group on metric name and that's going to make more sense for now. So I'm going to I'm just going to open a new query console here. Select star from array metrics right this has all of our data and we have three three three days of data right now right. But we want to aggregate this and what we want back here is dimensional analysis on three days and I'm just going to illustrate how this works kind of for and then it will make more sense how this works like for like a month. So we don't have to do the whole accumulation thing but so what we want to do is we want to say metric name and in this case we want to say some and then we want to say metric array at one right or and then we can say and then we put this back into an array. This is it's so weird like this is another thing this is another problem that I noticed that a lot of SQL syntax stuff doesn't work the right way for this. So now you'll see and then we can say group by metric array so this query works right there we go. So you see now how we have and then we have month start here right so we got about in the group I do month start. So you see how now we have like this is all added up though like how this is this is what we have one record here right so one of the things that you can do right is. This part can be like you can do like a it should be like a unnest like function here that gives this should okay that work so what you do is okay I know how to do it though so you have a the array not unnested right and we're going to call this as summed array. And we're going to say with we're say ag ag calls an aggregate for now. And then what we can do is we can say select star from from ag we can say cross join unnest and then in this case we have ag dot the summed array with ordinality right okay I think it works this way. With ordinality this is like this is getting absurdly fancy but I I assure you that this is important why we have this index here we're going to call this index though so if we query if we run this query. You'll see okay so now we have this index here right and postgres is dumb and we have to do minus one because it does one based indexing but we need to add one. Add one day to our month start so if we say so we have metric name then we say month start plus there should be like plus interval can you do like one day or like is it interval in yet we can say day index minus one. Is it like that what there should be a way to do that like add one day so the idea here is you can take this month and add one day to it like this is anyone know what the add one day function is like like from an int like date add or like I always we can get this stuff like date in date part date out date trunk. So problem here is remember it's zero based right so this is actually the wrong day so we need to do index minus one because postgres and okay perfect so now this is now we have our and then we have LM has value right and then this is okay there we go so now we have our date and we have you see how now this is back to daily aggregate right but the thing is is like I want to show how this works really well though because of like how how like if we just load in one more day into the metric right then this pops in and then over here all we got to do is add it to the array here and then this part was like I actually did a writing Python that generated this there needs to be a UDF that essentially just does this some for you but so you see now we have the fourth and that just added it very efficiently because the thing is is like this explode here is like we aren't like every user is only has one record right and so we can some everyone up and it's like very fast because we just sum up in the array and we don't ever explode the array we only explode the array after everything is aggregated and so that's why like we have like one record for each metric name but that's where like in here when you do when you have this this sum here this is where you could join in users to like get some other value right you could get like I know there's other values that you could get here to like explode out the dimensions and so that you would have more here but then you would have daily data with that dimensional value as well and so that's how you can go from monthly kind of array metrics back to daily aggregates but it's very fast because you have it's the minimal set of data that you need and that's like this idea like saved Facebook so much time and so much effort and energy congrats on finishing this five hour course doing all the hands on exercises and getting to the end and sticking it out I'm really proud of you congratulations not very many people get this far if you really like this content make sure to like comment and subscribe and good job I'm excited for you to check out week three